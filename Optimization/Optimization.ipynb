{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 2\n",
    "\n",
    "Generate random data from a predefined linear regression model where $−5 ≤ β_i ≤ 5$ $i =\n",
    "0, . . . , K$ with at least $K = 100$ independent variables $X = (X_1, X_2, . . . , X_K)$ and $n = 1000$\n",
    "observations. We seek to adjust a multiple linear regression model to explain variable Y as a\n",
    "function of the other variables X, i.e., $(Y = β\n",
    "_0X + \\epsilon)$ where $β = (β_0, β_1, . . . , β_K)$, by using\n",
    "“Ridge Regression”:\n",
    "\n",
    "\\begin{align*}\n",
    "&\\min_{\\beta} ||y − Xβ||_2^2 + ρ||β||_2^2\n",
    "\\\\\n",
    "\\end{align*}\n",
    "\n",
    "where ρ is a parameter (consider it fixed to a given value, e.g., ρ = 1).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a)  Estimate the value of the regression coefficients by implementing the analytical solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Values of the (exact) Ridge coefficients:\n",
      "beta   0  -1.209\n",
      "beta   1   0.999\n",
      "beta   2  -0.001\n",
      "beta   3  -1.002\n",
      "beta   4   2.999\n",
      "beta   5   4.001\n",
      "beta   6  -4.002\n",
      "beta   7   2.000\n",
      "beta   8   3.999\n",
      "beta   9   0.998\n",
      "beta  10   3.001\n",
      "beta  11  -4.998\n",
      "beta  12   0.000\n",
      "beta  13  -5.000\n",
      "beta  14   4.000\n",
      "beta  15   1.001\n",
      "beta  16  -3.001\n",
      "beta  17  -5.001\n",
      "beta  18  -0.001\n",
      "beta  19  -2.999\n",
      "beta  20   1.001\n",
      "beta  21  -1.998\n",
      "beta  22   2.000\n",
      "beta  23  -5.000\n",
      "beta  24   3.999\n",
      "beta  25  -5.000\n",
      "beta  26  -2.001\n",
      "beta  27  -3.002\n",
      "beta  28  -1.999\n",
      "beta  29  -3.998\n",
      "beta  30  -2.001\n",
      "beta  31  -4.000\n",
      "beta  32  -2.001\n",
      "beta  33   2.000\n",
      "beta  34  -4.000\n",
      "beta  35   2.001\n",
      "beta  36  -1.000\n",
      "beta  37  -5.001\n",
      "beta  38   0.002\n",
      "beta  39  -3.999\n",
      "beta  40  -0.001\n",
      "beta  41   4.001\n",
      "beta  42   3.999\n",
      "beta  43  -1.001\n",
      "beta  44  -5.000\n",
      "beta  45   4.000\n",
      "beta  46   2.999\n",
      "beta  47   2.998\n",
      "beta  48   0.999\n",
      "beta  49   3.000\n",
      "beta  50   1.001\n",
      "beta  51  -1.999\n",
      "beta  52  -4.000\n",
      "beta  53  -3.000\n",
      "beta  54  -0.000\n",
      "beta  55  -2.998\n",
      "beta  56   0.001\n",
      "beta  57   1.001\n",
      "beta  58   2.001\n",
      "beta  59  -0.998\n",
      "beta  60  -2.001\n",
      "beta  61  -0.001\n",
      "beta  62   0.999\n",
      "beta  63  -1.000\n",
      "beta  64   0.998\n",
      "beta  65  -3.000\n",
      "beta  66  -1.000\n",
      "beta  67  -3.000\n",
      "beta  68   1.999\n",
      "beta  69   3.999\n",
      "beta  70   1.999\n",
      "beta  71   2.000\n",
      "beta  72  -2.999\n",
      "beta  73   3.997\n",
      "beta  74   2.000\n",
      "beta  75  -0.998\n",
      "beta  76   4.001\n",
      "beta  77  -4.999\n",
      "beta  78   3.998\n",
      "beta  79  -3.001\n",
      "beta  80   3.999\n",
      "beta  81  -4.001\n",
      "beta  82  -3.002\n",
      "beta  83   4.002\n",
      "beta  84  -3.999\n",
      "beta  85  -0.000\n",
      "beta  86   2.000\n",
      "beta  87  -1.000\n",
      "beta  88   1.999\n",
      "beta  89   1.999\n",
      "beta  90  -3.998\n",
      "beta  91  -1.002\n",
      "beta  92  -4.999\n",
      "beta  93   0.001\n",
      "beta  94  -1.000\n",
      "beta  95   4.000\n",
      "beta  96  -3.001\n",
      "beta  97   3.999\n",
      "beta  98  -4.000\n",
      "beta  99  -2.001\n",
      "beta 100  -0.001\n",
      "Elapsed time =  0.00000\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy.linalg import inv\n",
    "np.random.seed(1234)\n",
    "\n",
    "K = 100\n",
    "beta = np.random.randint(-5,5,size=([K+1,1]));\n",
    "ntrain = 1000\n",
    "ntest = 100\n",
    "n = ntrain + ntest\n",
    "\n",
    "X0 = np.ones([n,1]) # the first column has all values equal to one for the coefficients of beta_0\n",
    "X1_s = np.random.uniform(0,100,([n,K]))\n",
    "X_s = np.concatenate([X0, X1_s],axis=1)\n",
    "\n",
    "## Values for the normal errors\n",
    "\n",
    "error_s = np.random.normal(0,1,(n,1))\n",
    "\n",
    "## Values for the y's\n",
    "\n",
    "Y_s = np.dot(X_s,beta) + error_s\n",
    "\n",
    "X = X_s[0:ntrain,:]\n",
    "Y = Y_s[0:ntrain]\n",
    "\n",
    "X_t = X_s[(ntrain+1):n,:]\n",
    "Y_t = Y_s[(ntrain+1):n]\n",
    "\n",
    "## Value of the regularization parameter\n",
    "rho = 1\n",
    "I = np.identity(K+1)\n",
    "\n",
    "time_start = time.process_time()\n",
    "\n",
    "beta_exact = np.dot(np.dot(inv(np.dot((X.T),X)+rho*I),X.T),Y)                   \n",
    "                   \n",
    "\n",
    "time_elapsed = (time.process_time() - time_start)\n",
    "\n",
    "## Print the results\n",
    "\n",
    "print('Values of the (exact) Ridge coefficients:')\n",
    "for i in range(K+1):\n",
    "    print('beta %3d %7.3f' %(i,beta_exact[i]))\n",
    "print('Elapsed time = %8.5f' %(time_elapsed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) Estimate the value of the regression coefficients by using the function minimize from the Python module $Scipy.optimize$. Try at least four available solvers and compare their performance in terms of number of iterations, number of function, gradient and hessian evaluations as well as total computational time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Maximum number of function evaluations has been exceeded.\n",
      "\n",
      "Values of the Ridge coefficients obtained with Nelder-Mead:\n",
      "beta   0  14.537\n",
      "beta   1   0.934\n",
      "beta   2   0.580\n",
      "beta   3   0.340\n",
      "beta   4   1.788\n",
      "beta   5  -4.884\n",
      "beta   6  -3.220\n",
      "beta   7  -0.827\n",
      "beta   8  -0.456\n",
      "beta   9  -5.606\n",
      "beta  10  -0.922\n",
      "beta  11   8.543\n",
      "beta  12  -0.535\n",
      "beta  13   1.635\n",
      "beta  14   1.486\n",
      "beta  15  -0.832\n",
      "beta  16  -0.736\n",
      "beta  17   1.919\n",
      "beta  18  -1.150\n",
      "beta  19   0.051\n",
      "beta  20   0.131\n",
      "beta  21  -3.397\n",
      "beta  22   4.323\n",
      "beta  23  -7.094\n",
      "beta  24  -1.911\n",
      "beta  25  -0.779\n",
      "beta  26  -0.779\n",
      "beta  27  -4.701\n",
      "beta  28   0.821\n",
      "beta  29   0.710\n",
      "beta  30   3.184\n",
      "beta  31  -0.859\n",
      "beta  32   1.788\n",
      "beta  33   0.867\n",
      "beta  34  -2.410\n",
      "beta  35  -1.173\n",
      "beta  36  -5.865\n",
      "beta  37  -1.344\n",
      "beta  38   2.411\n",
      "beta  39  -1.041\n",
      "beta  40  -0.295\n",
      "beta  41   0.055\n",
      "beta  42   8.598\n",
      "beta  43  -0.849\n",
      "beta  44  -1.757\n",
      "beta  45  -0.408\n",
      "beta  46   1.160\n",
      "beta  47   0.725\n",
      "beta  48  -1.065\n",
      "beta  49   1.221\n",
      "beta  50  -1.762\n",
      "beta  51  -1.005\n",
      "beta  52  -3.607\n",
      "beta  53  -1.077\n",
      "beta  54   5.110\n",
      "beta  55  -0.013\n",
      "beta  56  -1.350\n",
      "beta  57  -8.506\n",
      "beta  58   1.120\n",
      "beta  59  -0.394\n",
      "beta  60  -0.124\n",
      "beta  61  -1.566\n",
      "beta  62  -1.017\n",
      "beta  63   2.227\n",
      "beta  64   0.244\n",
      "beta  65  -3.092\n",
      "beta  66  -2.213\n",
      "beta  67  -4.794\n",
      "beta  68   0.265\n",
      "beta  69  -0.233\n",
      "beta  70   0.114\n",
      "beta  71   6.039\n",
      "beta  72   1.396\n",
      "beta  73  -0.862\n",
      "beta  74  -7.689\n",
      "beta  75   0.401\n",
      "beta  76  -1.059\n",
      "beta  77   4.676\n",
      "beta  78   5.599\n",
      "beta  79  -0.882\n",
      "beta  80   1.106\n",
      "beta  81  -0.166\n",
      "beta  82  -0.331\n",
      "beta  83  -1.266\n",
      "beta  84  -1.523\n",
      "beta  85  -5.099\n",
      "beta  86   3.952\n",
      "beta  87   0.435\n",
      "beta  88   0.077\n",
      "beta  89  -0.578\n",
      "beta  90   1.212\n",
      "beta  91  -0.078\n",
      "beta  92  -4.165\n",
      "beta  93   0.078\n",
      "beta  94  -0.703\n",
      "beta  95  -0.378\n",
      "beta  96   0.535\n",
      "beta  97  -0.044\n",
      "beta  98   3.058\n",
      "beta  99   1.173\n",
      "beta 100   0.142\n",
      "Elapsed time = 25.57812\n",
      "\n",
      "Error in values of coefficients =   1.4120\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 1843.480995\n",
      "         Iterations: 11\n",
      "         Function evaluations: 12\n",
      "         Gradient evaluations: 22\n",
      "         Hessian evaluations: 11\n",
      "\n",
      "Values of the Ridge coefficients obtained with Newton-CG:\n",
      "beta   0  14.537\n",
      "beta   1   0.934\n",
      "beta   2   0.580\n",
      "beta   3   0.340\n",
      "beta   4   1.788\n",
      "beta   5  -4.884\n",
      "beta   6  -3.220\n",
      "beta   7  -0.827\n",
      "beta   8  -0.456\n",
      "beta   9  -5.606\n",
      "beta  10  -0.922\n",
      "beta  11   8.543\n",
      "beta  12  -0.535\n",
      "beta  13   1.635\n",
      "beta  14   1.486\n",
      "beta  15  -0.832\n",
      "beta  16  -0.736\n",
      "beta  17   1.919\n",
      "beta  18  -1.150\n",
      "beta  19   0.051\n",
      "beta  20   0.131\n",
      "beta  21  -3.397\n",
      "beta  22   4.323\n",
      "beta  23  -7.094\n",
      "beta  24  -1.911\n",
      "beta  25  -0.779\n",
      "beta  26  -0.779\n",
      "beta  27  -4.701\n",
      "beta  28   0.821\n",
      "beta  29   0.710\n",
      "beta  30   3.184\n",
      "beta  31  -0.859\n",
      "beta  32   1.788\n",
      "beta  33   0.867\n",
      "beta  34  -2.410\n",
      "beta  35  -1.173\n",
      "beta  36  -5.865\n",
      "beta  37  -1.344\n",
      "beta  38   2.411\n",
      "beta  39  -1.041\n",
      "beta  40  -0.295\n",
      "beta  41   0.055\n",
      "beta  42   8.598\n",
      "beta  43  -0.849\n",
      "beta  44  -1.757\n",
      "beta  45  -0.408\n",
      "beta  46   1.160\n",
      "beta  47   0.725\n",
      "beta  48  -1.065\n",
      "beta  49   1.221\n",
      "beta  50  -1.762\n",
      "beta  51  -1.005\n",
      "beta  52  -3.607\n",
      "beta  53  -1.077\n",
      "beta  54   5.110\n",
      "beta  55  -0.013\n",
      "beta  56  -1.350\n",
      "beta  57  -8.506\n",
      "beta  58   1.120\n",
      "beta  59  -0.394\n",
      "beta  60  -0.124\n",
      "beta  61  -1.566\n",
      "beta  62  -1.017\n",
      "beta  63   2.227\n",
      "beta  64   0.244\n",
      "beta  65  -3.092\n",
      "beta  66  -2.213\n",
      "beta  67  -4.794\n",
      "beta  68   0.265\n",
      "beta  69  -0.233\n",
      "beta  70   0.114\n",
      "beta  71   6.039\n",
      "beta  72   1.396\n",
      "beta  73  -0.862\n",
      "beta  74  -7.689\n",
      "beta  75   0.401\n",
      "beta  76  -1.059\n",
      "beta  77   4.676\n",
      "beta  78   5.599\n",
      "beta  79  -0.882\n",
      "beta  80   1.106\n",
      "beta  81  -0.166\n",
      "beta  82  -0.331\n",
      "beta  83  -1.266\n",
      "beta  84  -1.523\n",
      "beta  85  -5.099\n",
      "beta  86   3.952\n",
      "beta  87   0.435\n",
      "beta  88   0.077\n",
      "beta  89  -0.578\n",
      "beta  90   1.212\n",
      "beta  91  -0.078\n",
      "beta  92  -4.165\n",
      "beta  93   0.078\n",
      "beta  94  -0.703\n",
      "beta  95  -0.378\n",
      "beta  96   0.535\n",
      "beta  97  -0.044\n",
      "beta  98   3.058\n",
      "beta  99   1.173\n",
      "beta 100   0.142\n",
      "Elapsed time =  0.20312\n",
      "\n",
      "Error in values of coefficients =   1.4120\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 1837.868397\n",
      "         Iterations: 116\n",
      "         Function evaluations: 235\n",
      "         Gradient evaluations: 235\n",
      "\n",
      "Values of the Ridge coefficients obtained with BFGS:\n",
      "beta   0  14.537\n",
      "beta   1   0.934\n",
      "beta   2   0.580\n",
      "beta   3   0.340\n",
      "beta   4   1.788\n",
      "beta   5  -4.884\n",
      "beta   6  -3.220\n",
      "beta   7  -0.827\n",
      "beta   8  -0.456\n",
      "beta   9  -5.606\n",
      "beta  10  -0.922\n",
      "beta  11   8.543\n",
      "beta  12  -0.535\n",
      "beta  13   1.635\n",
      "beta  14   1.486\n",
      "beta  15  -0.832\n",
      "beta  16  -0.736\n",
      "beta  17   1.919\n",
      "beta  18  -1.150\n",
      "beta  19   0.051\n",
      "beta  20   0.131\n",
      "beta  21  -3.397\n",
      "beta  22   4.323\n",
      "beta  23  -7.094\n",
      "beta  24  -1.911\n",
      "beta  25  -0.779\n",
      "beta  26  -0.779\n",
      "beta  27  -4.701\n",
      "beta  28   0.821\n",
      "beta  29   0.710\n",
      "beta  30   3.184\n",
      "beta  31  -0.859\n",
      "beta  32   1.788\n",
      "beta  33   0.867\n",
      "beta  34  -2.410\n",
      "beta  35  -1.173\n",
      "beta  36  -5.865\n",
      "beta  37  -1.344\n",
      "beta  38   2.411\n",
      "beta  39  -1.041\n",
      "beta  40  -0.295\n",
      "beta  41   0.055\n",
      "beta  42   8.598\n",
      "beta  43  -0.849\n",
      "beta  44  -1.757\n",
      "beta  45  -0.408\n",
      "beta  46   1.160\n",
      "beta  47   0.725\n",
      "beta  48  -1.065\n",
      "beta  49   1.221\n",
      "beta  50  -1.762\n",
      "beta  51  -1.005\n",
      "beta  52  -3.607\n",
      "beta  53  -1.077\n",
      "beta  54   5.110\n",
      "beta  55  -0.013\n",
      "beta  56  -1.350\n",
      "beta  57  -8.506\n",
      "beta  58   1.120\n",
      "beta  59  -0.394\n",
      "beta  60  -0.124\n",
      "beta  61  -1.566\n",
      "beta  62  -1.017\n",
      "beta  63   2.227\n",
      "beta  64   0.244\n",
      "beta  65  -3.092\n",
      "beta  66  -2.213\n",
      "beta  67  -4.794\n",
      "beta  68   0.265\n",
      "beta  69  -0.233\n",
      "beta  70   0.114\n",
      "beta  71   6.039\n",
      "beta  72   1.396\n",
      "beta  73  -0.862\n",
      "beta  74  -7.689\n",
      "beta  75   0.401\n",
      "beta  76  -1.059\n",
      "beta  77   4.676\n",
      "beta  78   5.599\n",
      "beta  79  -0.882\n",
      "beta  80   1.106\n",
      "beta  81  -0.166\n",
      "beta  82  -0.331\n",
      "beta  83  -1.266\n",
      "beta  84  -1.523\n",
      "beta  85  -5.099\n",
      "beta  86   3.952\n",
      "beta  87   0.435\n",
      "beta  88   0.077\n",
      "beta  89  -0.578\n",
      "beta  90   1.212\n",
      "beta  91  -0.078\n",
      "beta  92  -4.165\n",
      "beta  93   0.078\n",
      "beta  94  -0.703\n",
      "beta  95  -0.378\n",
      "beta  96   0.535\n",
      "beta  97  -0.044\n",
      "beta  98   3.058\n",
      "beta  99   1.173\n",
      "beta 100   0.142\n",
      "Elapsed time =  0.62500\n",
      "\n",
      "Error in values of coefficients =   1.4120\n",
      "\n",
      "Values of the least squares coefficients obtained with COBYLA for the Ridge problem:\n",
      "beta   0  14.537\n",
      "beta   1   0.934\n",
      "beta   2   0.580\n",
      "beta   3   0.340\n",
      "beta   4   1.788\n",
      "beta   5  -4.884\n",
      "beta   6  -3.220\n",
      "beta   7  -0.827\n",
      "beta   8  -0.456\n",
      "beta   9  -5.606\n",
      "beta  10  -0.922\n",
      "beta  11   8.543\n",
      "beta  12  -0.535\n",
      "beta  13   1.635\n",
      "beta  14   1.486\n",
      "beta  15  -0.832\n",
      "beta  16  -0.736\n",
      "beta  17   1.919\n",
      "beta  18  -1.150\n",
      "beta  19   0.051\n",
      "beta  20   0.131\n",
      "beta  21  -3.397\n",
      "beta  22   4.323\n",
      "beta  23  -7.094\n",
      "beta  24  -1.911\n",
      "beta  25  -0.779\n",
      "beta  26  -0.779\n",
      "beta  27  -4.701\n",
      "beta  28   0.821\n",
      "beta  29   0.710\n",
      "beta  30   3.184\n",
      "beta  31  -0.859\n",
      "beta  32   1.788\n",
      "beta  33   0.867\n",
      "beta  34  -2.410\n",
      "beta  35  -1.173\n",
      "beta  36  -5.865\n",
      "beta  37  -1.344\n",
      "beta  38   2.411\n",
      "beta  39  -1.041\n",
      "beta  40  -0.295\n",
      "beta  41   0.055\n",
      "beta  42   8.598\n",
      "beta  43  -0.849\n",
      "beta  44  -1.757\n",
      "beta  45  -0.408\n",
      "beta  46   1.160\n",
      "beta  47   0.725\n",
      "beta  48  -1.065\n",
      "beta  49   1.221\n",
      "beta  50  -1.762\n",
      "beta  51  -1.005\n",
      "beta  52  -3.607\n",
      "beta  53  -1.077\n",
      "beta  54   5.110\n",
      "beta  55  -0.013\n",
      "beta  56  -1.350\n",
      "beta  57  -8.506\n",
      "beta  58   1.120\n",
      "beta  59  -0.394\n",
      "beta  60  -0.124\n",
      "beta  61  -1.566\n",
      "beta  62  -1.017\n",
      "beta  63   2.227\n",
      "beta  64   0.244\n",
      "beta  65  -3.092\n",
      "beta  66  -2.213\n",
      "beta  67  -4.794\n",
      "beta  68   0.265\n",
      "beta  69  -0.233\n",
      "beta  70   0.114\n",
      "beta  71   6.039\n",
      "beta  72   1.396\n",
      "beta  73  -0.862\n",
      "beta  74  -7.689\n",
      "beta  75   0.401\n",
      "beta  76  -1.059\n",
      "beta  77   4.676\n",
      "beta  78   5.599\n",
      "beta  79  -0.882\n",
      "beta  80   1.106\n",
      "beta  81  -0.166\n",
      "beta  82  -0.331\n",
      "beta  83  -1.266\n",
      "beta  84  -1.523\n",
      "beta  85  -5.099\n",
      "beta  86   3.952\n",
      "beta  87   0.435\n",
      "beta  88   0.077\n",
      "beta  89  -0.578\n",
      "beta  90   1.212\n",
      "beta  91  -0.078\n",
      "beta  92  -4.165\n",
      "beta  93   0.078\n",
      "beta  94  -0.703\n",
      "beta  95  -0.378\n",
      "beta  96   0.535\n",
      "beta  97  -0.044\n",
      "beta  98   3.058\n",
      "beta  99   1.173\n",
      "beta 100   0.142\n",
      "Elapsed time = 10.73438\n",
      "\n",
      "Error in values of coefficients =   1.4120\n"
     ]
    }
   ],
   "source": [
    " from scipy.optimize import minimize\n",
    "from numpy.linalg import norm\n",
    "\n",
    "# Definition of the optimization problem\n",
    "rho = 1\n",
    "\n",
    "def ridge_reg(beta_ridge,X,Y, rho):\n",
    "    beta_ridge = np.matrix(beta_ridge)\n",
    "   # print(\"Tamano de X %d \" % len(X))\n",
    "    z = Y - np.dot(X,beta_ridge.T)\n",
    "   # print(\"Tamano de Z %d \" % len(z))\n",
    "    val = np.dot(z.T,z) + rho*np.power(np.linalg.norm(beta_ridge),2)\n",
    "    #val = np.dot(z.T,z) + np.dot(beta_ridge,beta_ridge.T)\n",
    "    return val \n",
    "                   \n",
    "# Define the first and second derivatives of the problem\n",
    "\n",
    "def ridge_reg_der(beta_ridge,X,Y, rho):\n",
    "    beta_ridge = np.matrix(beta_ridge)\n",
    "    pp=-2*np.dot((Y-np.dot(X,beta_ridge.T)).T,X)+2*rho*beta_ridge\n",
    "    aa=np.squeeze(np.asarray(pp))\n",
    "    return aa\n",
    "\n",
    "#Hessian\n",
    "\n",
    "def ridge_hess(beta_ridge,X,Y, rho):\n",
    "    ss = 2*np.dot(np.transpose(X),X)+2*rho*np.identity(101)\n",
    "    return ss\n",
    " \n",
    "beta_0 = np.zeros(K+1)\n",
    "                 \n",
    "## We solve the optimization algorithm using a derivative-free method   \n",
    "rho = 1                  \n",
    "time_start = time.process_time()\n",
    "\n",
    "res = minimize(ridge_reg, beta_0, args=(X, Y, rho), method='Nelder-Mead', options={'disp': True,'xtol': 1e-10})\n",
    "                   \n",
    "time_elapsed = (time.process_time() - time_start) \n",
    "                   \n",
    "print('\\nValues of the Ridge coefficients obtained with Nelder-Mead:')\n",
    "for i in range(K+1):\n",
    "    print('beta %3d %7.3f' %(i,res.x[i]))\n",
    "print('Elapsed time = %8.5f' %(time_elapsed)) \n",
    "\n",
    "err_val_1 = np.linalg.norm(np.transpose(beta_exact)-res.x,ord=2)/np.linalg.norm(np.transpose(beta_exact),ord=2)\n",
    "print('\\nError in values of coefficients = %8.4f' %err_val_1)\n",
    "                   \n",
    "## Using a variant of Newton's method  \n",
    "                   \n",
    "time_start2 = time.process_time()                   \n",
    "                   \n",
    "res2 = minimize(ridge_reg, beta_0, args=(X, Y, rho), method='Newton-CG', jac=ridge_reg_der, hess=ridge_hess, options={'disp': True})\n",
    "\n",
    "time_elapsed2 = (time.process_time() - time_start2)\n",
    "\n",
    "print('\\nValues of the Ridge coefficients obtained with Newton-CG:')\n",
    "for i in range(K+1):\n",
    "    print('beta %3d %7.3f' %(i,res.x[i]))\n",
    "print('Elapsed time = %8.5f' %(time_elapsed2))\n",
    "                   \n",
    "err_val_2 = np.linalg.norm(np.transpose(beta_exact)-res.x,ord=2)/np.linalg.norm(np.transpose(beta_exact),ord=2)\n",
    "print('\\nError in values of coefficients = %8.4f' %err_val_2)\n",
    "\n",
    "## Using a Quasi-Newton's method     \n",
    "                   \n",
    "time_start3 = time.process_time()                   \n",
    "                                      \n",
    "res3 = minimize(ridge_reg, beta_0, args=(X, Y, rho), method='BFGS', jac=ridge_reg_der, options={'disp': True})\n",
    "                   \n",
    "time_elapsed3 = (time.process_time() - time_start3) \n",
    "                   \n",
    "print('\\nValues of the Ridge coefficients obtained with BFGS:')\n",
    "for i in range(K+1):\n",
    "    print('beta %3d %7.3f' %(i,res.x[i]))\n",
    "print('Elapsed time = %8.5f' %(time_elapsed3))\n",
    "\n",
    "err_val_3 = np.linalg.norm(np.transpose(beta_exact)-res.x,ord=2)/np.linalg.norm(np.transpose(beta_exact),ord=2)\n",
    "print('\\nError in values of coefficients = %8.4f' %err_val_3)\n",
    "\n",
    "## Using a sequential quadratic programming algorithm   \n",
    "                   \n",
    "time_start4 = time.process_time()                   \n",
    "\n",
    "res4 = minimize(ridge_reg, beta_0, args=(X, Y, rho), method='COBYLA', options={'disp': True})\n",
    "                   \n",
    "time_elapsed4 = (time.process_time() - time_start4)   \n",
    "\n",
    "print('\\nValues of the least squares coefficients obtained with COBYLA for the Ridge problem:')\n",
    "for i in range(K+1):\n",
    "    print('beta %3d %7.3f' %(i,res.x[i]))\n",
    "print('Elapsed time = %8.5f' %(time_elapsed4))\n",
    "\n",
    "err_val_4 = np.linalg.norm(np.transpose(beta_exact)-res.x,ord=2)/np.linalg.norm(np.transpose(beta_exact),ord=2)\n",
    "print('\\nError in values of coefficients = %8.4f' %err_val_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c) Estimate the value of the regression coefficients by implementing the: \n",
    "\n",
    "#### Consider a line search technique to improve the algorithm convergence, e.x., Armijo rule. Compare the performance of these algorithms (number of iterations and total computational time)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### i. Gradient method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steepest descent method: We start from an initial iterate $x_0$, and compute search descent directions $p_k=-\\nabla f(x_k)$. Far from the solution, compute a steplength $\\alpha_k>0$\n",
    "The initial iterate $x_0$ is moved the following way:\n",
    "$$x_{k+1} = x_k + \\alpha_k\\ p_k$$ until it converges to a local solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed computational time = 63.64062\n",
      "\n",
      "Number of iterations = 49999\n",
      "Objective function   =  6966.46803\n",
      "Optimality tolerance = 96650.90766\n",
      "\n",
      "Values of the least squares coefficients with the gradient method:\n",
      "beta intercept  -0.007\n",
      "beta 1           1.007\n",
      "beta 2           0.008\n",
      "beta 3          -0.994\n",
      "beta 4           2.982\n",
      "beta 5           3.996\n",
      "beta 6          -3.994\n",
      "\n",
      "Beta coefficient error =    0.04101\n"
     ]
    }
   ],
   "source": [
    "# Implementation of the gradient method\n",
    "from numpy.linalg import norm\n",
    "\n",
    "(a,b) = X.shape\n",
    "\n",
    "## Algorithm's parameters\n",
    "\n",
    "alpha = 1e-10 #steplenght\n",
    "n_it = 50000  #number of iterations\n",
    "epsilon = 1e-5 \n",
    "tol = 10000 #tolerance\n",
    "\n",
    "## Initial values for the variables and data containers\n",
    "\n",
    "beta_gradient = np.zeros(b) \n",
    "OF_it = np.zeros(n_it)\n",
    "tol_it = np.zeros(n_it)\n",
    "alpha_it = np.zeros(n_it)\n",
    "\n",
    "# We implement the gradient method\n",
    "\n",
    "start = time.process_time()\n",
    "\n",
    "i = 0\n",
    "\n",
    "while (i <= n_it-2) and (tol > epsilon):\n",
    "    i = i + 1\n",
    "    grad = ridge_reg_der(beta_gradient,X,Y, rho) # Gradient vector\n",
    "    ddirect = -grad # Descent direction\n",
    "    beta_gradient = beta_gradient + alpha*ddirect\n",
    "    \n",
    "    OF_it[i] = ridge_reg(beta_gradient, X, Y, rho)\n",
    "    tol = np.linalg.norm(grad,ord=2)\n",
    "    tol_it[i] = tol\n",
    "    alpha_it[i] = alpha\n",
    "    \n",
    "total_time = (time.process_time() - start)\n",
    "\n",
    "print('Elapsed computational time = %8.5f' %(total_time))\n",
    "print('\\nNumber of iterations = %5.0f' %i)\n",
    "print('Objective function   = %11.5f' %OF_it[i])\n",
    "print('Optimality tolerance = %11.5f' %tol)\n",
    "print('\\nValues of the least squares coefficients with the gradient method:')\n",
    "print('beta %-9s %7.3f' %('intercept',beta_gradient[0]))\n",
    "print('beta %-9s %7.3f' %('1',beta_gradient[1]))\n",
    "print('beta %-9s %7.3f' %('2',beta_gradient[2]))\n",
    "print('beta %-9s %7.3f' %('3',beta_gradient[3]))\n",
    "print('beta %-9s %7.3f' %('4',beta_gradient[4]))\n",
    "print('beta %-9s %7.3f' %('5',beta_gradient[5]))\n",
    "print('beta %-9s %7.3f' %('6',beta_gradient[6]))\n",
    "\n",
    "beta_error = np.linalg.norm(np.transpose(beta_exact)-beta_gradient,ord=2)/np.linalg.norm(beta_gradient,ord=2)\n",
    "print('\\nBeta coefficient error = %10.5f' %beta_error)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ii. Newton method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Start from an initial iterate $x_0$, compute a search descent direction \n",
    "$$p_k=-(\\nabla^2 f(x_k))^{-1} \\nabla f(x_k)$$\n",
    "\n",
    "such that $\\nabla^2 f(x_k)$ is nonsingular. Far away from the solution, compute a steplength $\\alpha_k>0$\n",
    "\n",
    "From the initial iterate, move in the following direction $$x_{k+1} = x_k + \\alpha_k\\ p_k$$\n",
    "until it converges to a local solution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed computational time =  2.73438\n",
      "\n",
      "Number of iterations =   199\n",
      "Objective function   =  1837.86840\n",
      "Optimality tolerance =     0.00001\n",
      "\n",
      "Values of the least squares coefficients:\n",
      "beta intercept  -1.209\n",
      "beta 1           0.999\n",
      "beta 2          -0.001\n",
      "beta 3          -1.002\n",
      "beta 4           2.999\n",
      "beta 5           4.001\n",
      "beta 6          -4.002\n",
      "\n",
      "Beta coefficient error =    0.00000\n"
     ]
    }
   ],
   "source": [
    "(a,b) = X.shape\n",
    "\n",
    "## Parameters \n",
    "\n",
    "alpha = 1e-6\n",
    "n_it = 200 \n",
    "epsilon = 1e-6\n",
    "tol = 100\n",
    "sigma = 0.2\n",
    "delta = 0.1\n",
    "\n",
    "## Initial values for the variables and data containers\n",
    "\n",
    "beta_newton = np.zeros(b) # initial value for beta\n",
    "\n",
    "OF_it = np.zeros(n_it)\n",
    "tol_it = np.zeros(n_it)\n",
    "alpha_it = np.zeros(n_it)\n",
    "\n",
    "# Implement Newton's method\n",
    "\n",
    "start = time.process_time()\n",
    "\n",
    "i = 0\n",
    "\n",
    "while (i <= n_it-2) and (tol > epsilon):\n",
    "    i = i + 1\n",
    "    grad = ridge_reg_der(beta_newton,X,Y, rho)\n",
    "    hess = ridge_hess(beta_newton,X,Y, rho)\n",
    "    ddirect = -np.dot(np.linalg.inv(hess),grad) \n",
    "    \n",
    "#We use the Armijo rule to adjust alpha and improve algorithm convergence\n",
    "    alpha=1\n",
    "    while (ridge_reg(beta_newton+alpha*ddirect,X,Y, rho) > ridge_reg(beta_newton,X,Y, rho)+alpha*sigma*np.dot(ddirect,grad)):\n",
    "        alpha = alpha*delta\n",
    "\n",
    "    beta_newton = beta_newton + alpha*ddirect\n",
    "    OF_it[i] = ridge_reg(beta_newton, X, Y, rho)\n",
    "    tol = np.linalg.norm(grad,ord=2)\n",
    "    tol_it[i] = tol\n",
    "    alpha_it[i] = alpha\n",
    "\n",
    "total_time = (time.process_time() - start)\n",
    "\n",
    "## Print the results\n",
    "\n",
    "print('Elapsed computational time = %8.5f' %(total_time))\n",
    "print('\\nNumber of iterations = %5.0f' %i)\n",
    "print('Objective function   = %11.5f' %OF_it[i])\n",
    "print('Optimality tolerance = %11.5f' %tol)\n",
    "\n",
    "print('\\nValues of the least squares coefficients:')\n",
    "print('beta %-9s %7.3f' %('intercept',beta_newton[0]))\n",
    "print('beta %-9s %7.3f' %('1',beta_newton[1]))\n",
    "print('beta %-9s %7.3f' %('2',beta_newton[2]))\n",
    "print('beta %-9s %7.3f' %('3',beta_newton[3]))\n",
    "print('beta %-9s %7.3f' %('4',beta_newton[4]))\n",
    "print('beta %-9s %7.3f' %('5',beta_newton[5]))\n",
    "print('beta %-9s %7.3f' %('6',beta_newton[6]))\n",
    "\n",
    "beta_error = np.linalg.norm(np.transpose(beta_exact)-beta_newton,ord=2)/np.linalg.norm(beta_newton,ord=2)\n",
    "print('\\nBeta coefficient error = %10.5f' %beta_error)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### iii. Quasi-Newton method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start from an initial approximation $B_0$ to $H_0 = \\nabla^2 f(x_0)$ and move it as:\n",
    "\n",
    "$$B_{k+1}=B_k+ U$$\n",
    "\n",
    "With U an update rule with to versions:\n",
    "\n",
    "-Symmetric rank-one: \n",
    "$$U = \\frac{(y_k-B_k s_k)(y_k-B_k s_k)^T)}{(y_k-B_k s_k)^T s_k}$$\n",
    "\n",
    "-BFGS:\n",
    "$$U = \\frac{(B_k s_k)(B_k s_k)^T}{s_k^T B_k s_k}+\\frac{y_k y_k^T}{y_k^T s_k}$$\n",
    "\n",
    "with $s_k = x_{k+1}-x_k$, $y_k = \\nabla f(x_{k+1})-\\nabla f(x_{k})$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed computational time =  0.12500\n",
      "\n",
      "Number of iterations =   100\n",
      "Objective function   =     0.00000\n",
      "Optimality tolerance = 41069956.90801\n",
      "\n",
      "Values of the least squares coefficients:\n",
      "beta intercept  -0.000\n",
      "beta 1           2.562\n",
      "beta 2           0.916\n",
      "beta 3           0.170\n",
      "beta 4           5.168\n",
      "beta 5           6.217\n",
      "beta 6          -4.386\n",
      "\n",
      "Beta coefficient error =    0.40264\n"
     ]
    }
   ],
   "source": [
    "(a,b) = X.shape\n",
    "\n",
    "## Parameters \n",
    "\n",
    "alpha = 1e-3\n",
    "n_it = 101\n",
    "epsilon = 1e-7\n",
    "tol = 100\n",
    "sigma = 0.2\n",
    "delta = 0.1\n",
    "\n",
    "## Initial values for the variables and data containers\n",
    "\n",
    "beta_quasi = np.zeros(b) # initial value for beta\n",
    "\n",
    "OF_it = np.zeros(n_it)\n",
    "tol_it = np.zeros(n_it)\n",
    "alpha_it = np.zeros(n_it)\n",
    "s = np.zeros(b)\n",
    "yk = np.zeros(n_it)\n",
    "h = np.zeros(n_it)\n",
    "t = np.zeros(n_it)\n",
    "r = np.zeros(n_it)\n",
    "u = np.zeros(n_it)\n",
    "\n",
    "\n",
    "# Implement Quasi-Newton's method with the symmetric rank-one update rule:\n",
    "\n",
    "start = time.process_time()\n",
    "\n",
    "i = 0\n",
    "\n",
    "for i in range(1, len(beta_quasi)):\n",
    "    if tol <= epsilon:\n",
    "        break\n",
    "#while (i <= n_it-2) and (tol > epsilon):\n",
    "        #i = i + 1\n",
    "        grad = ridge_reg_der(beta_quasi,X,Y, rho)\n",
    "        hess = ridge_hess(beta_quasi,X,Y, rho)\n",
    "        ddirect = -np.dot(np.linalg.inv(hess),grad)\n",
    "        \n",
    "        alpha=1\n",
    "    while (ridge_reg(beta_newton+alpha*ddirect,X,Y, rho) > ridge_reg(beta_newton,X,Y, rho)+alpha*sigma*np.dot(ddirect,grad)):\n",
    "        alpha = alpha*delta\n",
    "        \n",
    "        #beta_quasi = beta_quasi + alpha*ddirect\n",
    "        s[i] = beta_quasi[i]-beta_quasi[i-1]\n",
    "        yk = grad[i]-grad[i-1]\n",
    "        h = np.dot(hess,s)\n",
    "        t = np.dot((yk-h),(yk-h).T)\n",
    "        r = np.power(np.dot(((yk-h).T),s),-1)\n",
    "        u = np.dot(t,r)\n",
    "\n",
    "        hess[i] = hess[i-1] + u\n",
    "        beta_quasi = beta_quasi + alpha*ddirect\n",
    "        OF_it[i] = ridge_reg(beta_quasi, X, Y, rho)\n",
    "        tol = np.linalg.norm(grad,ord=2)\n",
    "        tol_it[i] = tol\n",
    "        alpha_it[i] = alpha\n",
    "\n",
    "total_time = (time.process_time() - start)    \n",
    "print('Elapsed computational time = %8.5f' %(total_time))\n",
    "print('\\nNumber of iterations = %5.0f' %i)\n",
    "print('Objective function   = %11.5f' %OF_it[i])\n",
    "print('Optimality tolerance = %11.5f' %tol)\n",
    "print('\\nValues of the least squares coefficients:')\n",
    "print('beta %-9s %7.3f' %('intercept',beta_quasi[0]))\n",
    "print('beta %-9s %7.3f' %('1',beta_quasi[1]))\n",
    "print('beta %-9s %7.3f' %('2',beta_quasi[2]))\n",
    "print('beta %-9s %7.3f' %('3',beta_quasi[3]))\n",
    "print('beta %-9s %7.3f' %('4',beta_quasi[4]))\n",
    "print('beta %-9s %7.3f' %('5',beta_quasi[5]))\n",
    "print('beta %-9s %7.3f' %('6',beta_quasi[6]))\n",
    "\n",
    "beta_error = np.linalg.norm(np.transpose(beta_exact)-beta_quasi,ord=2)/np.linalg.norm(beta_quasi,ord=2)\n",
    "print('\\nBeta coefficient error = %10.5f' %beta_error)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d) Estimate the value of the regression coefficients by implementing the:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### i. Coordinate gradient method.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just modify $x_i$ to improve the objective function and:\n",
    "\n",
    "1: Choose an index $i_k \\in \\{1,2,\\dots,p\\}$ randomly\n",
    "\n",
    "2: $x^{k+1}=x^{k}-\\alpha\\nabla_{i_{k}}f(x^{k})e_{ik}$\n",
    "\n",
    "where $e_i$ is the $i^\\text{th}$ canonical coordinate vector and $\\nabla_{i}f(x)$\n",
    "is the $i^\\text{th}$ coordinate of the gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of iterations 49999\n",
      "[[ 8.40844682e-03]\n",
      " [ 9.98630938e-01]\n",
      " [-1.70836957e-03]\n",
      " [-1.00212784e+00]\n",
      " [ 2.99903849e+00]\n",
      " [ 4.00045260e+00]\n",
      " [-4.00187690e+00]\n",
      " [ 1.99942507e+00]\n",
      " [ 3.99883564e+00]\n",
      " [ 9.98227525e-01]\n",
      " [ 3.00034099e+00]\n",
      " [-4.99845997e+00]\n",
      " [-1.18891025e-04]\n",
      " [-5.00001329e+00]\n",
      " [ 3.99957261e+00]\n",
      " [ 1.00048692e+00]\n",
      " [-3.00119243e+00]\n",
      " [-5.00167722e+00]\n",
      " [-1.10884056e-03]\n",
      " [-2.99967869e+00]\n",
      " [ 1.00093688e+00]\n",
      " [-1.99831956e+00]\n",
      " [ 1.99987384e+00]\n",
      " [-5.00042780e+00]\n",
      " [ 3.99841544e+00]\n",
      " [-5.00003133e+00]\n",
      " [-2.00065150e+00]\n",
      " [-3.00257748e+00]\n",
      " [-1.99901673e+00]\n",
      " [-3.99845338e+00]\n",
      " [-2.00087486e+00]\n",
      " [-4.00027867e+00]\n",
      " [-2.00146812e+00]\n",
      " [ 1.99952157e+00]\n",
      " [-4.00024980e+00]\n",
      " [ 2.00039452e+00]\n",
      " [-1.00011054e+00]\n",
      " [-5.00121837e+00]\n",
      " [ 2.11951250e-03]\n",
      " [-3.99951275e+00]\n",
      " [-1.70055504e-03]\n",
      " [ 4.00081244e+00]\n",
      " [ 3.99915665e+00]\n",
      " [-1.00115250e+00]\n",
      " [-4.99972763e+00]\n",
      " [ 3.99974787e+00]\n",
      " [ 2.99907147e+00]\n",
      " [ 2.99824275e+00]\n",
      " [ 9.99087570e-01]\n",
      " [ 3.00016717e+00]\n",
      " [ 1.00088857e+00]\n",
      " [-1.99928320e+00]\n",
      " [-3.99989124e+00]\n",
      " [-3.00019696e+00]\n",
      " [-5.49555641e-04]\n",
      " [-2.99863850e+00]\n",
      " [ 4.45477639e-04]\n",
      " [ 1.00034791e+00]\n",
      " [ 2.00045331e+00]\n",
      " [-9.98460669e-01]\n",
      " [-2.00156752e+00]\n",
      " [-8.63737289e-04]\n",
      " [ 9.98825688e-01]\n",
      " [-9.99836849e-01]\n",
      " [ 9.97931010e-01]\n",
      " [-3.00054337e+00]\n",
      " [-1.00011008e+00]\n",
      " [-3.00024740e+00]\n",
      " [ 1.99885149e+00]\n",
      " [ 3.99884413e+00]\n",
      " [ 1.99901873e+00]\n",
      " [ 2.00021785e+00]\n",
      " [-2.99914620e+00]\n",
      " [ 3.99661303e+00]\n",
      " [ 1.99921036e+00]\n",
      " [-9.97898697e-01]\n",
      " [ 4.00064369e+00]\n",
      " [-4.99924543e+00]\n",
      " [ 3.99783728e+00]\n",
      " [-3.00096881e+00]\n",
      " [ 3.99839436e+00]\n",
      " [-4.00105476e+00]\n",
      " [-3.00211229e+00]\n",
      " [ 4.00135876e+00]\n",
      " [-3.99931251e+00]\n",
      " [-6.97415709e-04]\n",
      " [ 2.00006617e+00]\n",
      " [-9.99947227e-01]\n",
      " [ 1.99819379e+00]\n",
      " [ 1.99905548e+00]\n",
      " [-3.99866879e+00]\n",
      " [-1.00188026e+00]\n",
      " [-4.99946386e+00]\n",
      " [ 6.76334555e-04]\n",
      " [-1.00013457e+00]\n",
      " [ 3.99938866e+00]\n",
      " [-3.00113806e+00]\n",
      " [ 3.99860072e+00]\n",
      " [-3.99977000e+00]\n",
      " [-2.00119345e+00]\n",
      " [-1.52976819e-03]]\n",
      "Tolerance= 100\n",
      "Error= 0.04132458164880103\n"
     ]
    }
   ],
   "source": [
    "#First, define the coordinated gradient\n",
    "def ridge_reg(beta_ridge,X,Y,rho):\n",
    "    beta_ridge = np.matrix(beta_ridge)\n",
    "    z = Y - np.dot(X,beta_ridge.T)\n",
    "    val = np.dot(z.T,z) + rho*np.dot(beta_ridge,beta_ridge.T)\n",
    "    return val \n",
    "\n",
    "def ridge_der_coord(beta_ridge,index,X,Y):\n",
    "    beta_ridge = np.matrix(beta_ridge)\n",
    "    pp=-2*np.dot((Y-np.dot(X,beta_ridge)).T,X[:,index])\n",
    "    #pp=np.array(-2*np.dot((Y-np.dot(X,beta_ls)).T,X[:,index]))\n",
    "    aa=np.zeros([b,1])\n",
    "    aa[index]=pp\n",
    "    return aa\n",
    "\n",
    "#Define the parameters\n",
    "\n",
    "(a,b)=X.shape\n",
    "beta_ridge=np.zeros([b,1])\n",
    "alpha=1e-7\n",
    "n_it=50000\n",
    "OF_it=np.zeros(n_it)\n",
    "i=0;\n",
    "tol=100;\n",
    "epsilon=1e-6;\n",
    "\n",
    "#And apply the algorithm\n",
    "\n",
    "while (i <= n_it-2):\n",
    "    i=i+1\n",
    "    k = np.random.randint(b)\n",
    "    grad_k = ridge_der_coord(beta_ridge,k,X,Y)\n",
    "    ddirect=-grad_k\n",
    "    beta_ridge=beta_ridge+alpha*ddirect\n",
    "    #OF_it[i]=ridge_reg(beta_ridge, X, Y)\n",
    "print('Number of iterations',i)\n",
    "#print(OF_it[i])\n",
    "print(beta_ridge)\n",
    "print('Tolerance=',tol)\n",
    "print('Error=',np.linalg.norm(beta_exact-beta_ridge,ord=2)/np.linalg.norm(beta_ridge,ord=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ii. Mini-batch gradient method. Study how the mini-batch size may impact the algorithm performance (number of iterations and computational time needed to reach a pre-specified tolerance limit).\n",
    "\n",
    "Consider at each step a sample of size $b$, and iterate in $k$ selecting a random sample $(x_i, y_i)$, $i \\in S_k$, and a gradient as\n",
    "\n",
    "$$g_k \\approx \\frac{1}{b} \\sum_{i\\in S_k} \\nabla F(w_k;(x_i, y_i))$$\n",
    "\n",
    "Commonly, mini-batch sizes, $b=|S_k|$, range between 50 and 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed computational time = 57.25000\n",
      "\n",
      "Number of iterations = 49999\n",
      "Objective function   =  1840.71280\n",
      "Optimality tolerance =     9.47814\n",
      "\n",
      "Values of the least squares coefficients with the mini-batch method:\n",
      "beta intercept  -0.008\n",
      "beta 1           0.999\n",
      "beta 2          -0.002\n",
      "beta 3          -1.002\n",
      "beta 4           2.999\n",
      "beta 5           4.000\n",
      "beta 6          -4.002\n",
      "\n",
      "Beta coefficient error =    0.04077\n"
     ]
    }
   ],
   "source": [
    "#Define the parameters\n",
    "\n",
    "(a,b)=X1.shape\n",
    "alpha=1e-7\n",
    "n_it=50000\n",
    "tol=100;\n",
    "epsilon=1e-6;\n",
    "\n",
    " \n",
    "tol_it = np.zeros(n_it)\n",
    "alpha_it = np.zeros(n_it)\n",
    "OF_it=np.zeros(n_it)\n",
    "\n",
    "X1 = X_s[0:200,:]\n",
    "X2 = X_s[201:400,:]\n",
    "X3 = X_s[401:600,:]\n",
    "X4 = X_s[601:800,:]\n",
    "X5 = X_s[801:1000,:] \n",
    "Y1 = Y_s[0:200,:]\n",
    "Y2 = Y_s[201:400,:]\n",
    "Y3 = Y_s[401:600,:]\n",
    "Y4 = Y_s[601:800,:]\n",
    "Y5 = Y_s[801:1000,:]\n",
    "(a,b)=X1.shape #200x101 matrix\n",
    "h = len(X1)\n",
    "beta_batch = np.zeros(b)\n",
    "\n",
    "P = np.concatenate([X1, X2, X3, X4, X5], axis=0)\n",
    "Q = np.concatenate([Y1, Y2, Y3, Y4, Y5], axis=0)\n",
    "\n",
    "# We implement the method\n",
    "\n",
    "start = time.process_time()\n",
    "\n",
    "i = 0\n",
    "\n",
    "    \n",
    "\n",
    "while (i <= n_it-2) and (tol > epsilon):\n",
    "    i = i + 1\n",
    "    grad = ridge_reg_der(beta_batch,P,Q, rho) # Gradient vector\n",
    "    ddirect = -grad/b # Descent direction\n",
    "    #g = grad/b \n",
    "    beta_batch = beta_batch + alpha*ddirect\n",
    "    \n",
    "    OF_it[i] = ridge_reg(beta_batch, P, Q, rho)\n",
    "    tol = np.linalg.norm(grad,ord=2)\n",
    "    tol_it[i] = tol\n",
    "    alpha_it[i] = alpha\n",
    "    \n",
    "total_time = (time.process_time() - start)\n",
    "\n",
    "\n",
    "print('Elapsed computational time = %8.5f' %(total_time))\n",
    "print('\\nNumber of iterations = %5.0f' %i)\n",
    "print('Objective function   = %11.5f' %OF_it[i])\n",
    "print('Optimality tolerance = %11.5f' %tol)\n",
    "print('\\nValues of the least squares coefficients with the mini-batch method:')\n",
    "print('beta %-9s %7.3f' %('intercept',beta_batch[0]))\n",
    "print('beta %-9s %7.3f' %('1',beta_batch[1]))\n",
    "print('beta %-9s %7.3f' %('2',beta_batch[2]))\n",
    "print('beta %-9s %7.3f' %('3',beta_batch[3]))\n",
    "print('beta %-9s %7.3f' %('4',beta_batch[4]))\n",
    "print('beta %-9s %7.3f' %('5',beta_batch[5]))\n",
    "print('beta %-9s %7.3f' %('6',beta_batch[6]))\n",
    "\n",
    "beta_error = np.linalg.norm(np.transpose(beta_exact)-beta_batch,ord=2)/np.linalg.norm(beta_batch,ord=2)\n",
    "print('\\nBeta coefficient error = %10.5f' %beta_error)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### iii. Mini-batch gradient with momentum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, a moment is added in order to add information from past iterations:\n",
    "\n",
    "$$v_tx_t←\\beta v_t−1+g_{t,t-1}$$\n",
    "$$x_t←x_{t−1}−\\eta_tv_t$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed computational time =  0.18750\n",
      "\n",
      "Number of iterations =   100\n",
      "Objective function   = 492332410.66486\n",
      "Optimality tolerance = 40899726.99627\n",
      "\n",
      "Values of the least squares coefficients with the mini-batch method with momentum:\n",
      "beta intercept  -0.007\n",
      "beta 1          -0.106\n",
      "beta 2          -0.268\n",
      "beta 3          -0.330\n",
      "beta 4           0.157\n",
      "beta 5           0.256\n",
      "beta 6          -0.780\n",
      "\n",
      "Beta coefficient error =    4.38301\n"
     ]
    }
   ],
   "source": [
    "#Define the parameters\n",
    "\n",
    "#(a,b)=X1.shape\n",
    "alpha=1e-7\n",
    "n_it=101\n",
    "tol=100;\n",
    "epsilon=1e-7;\n",
    "eta=1e-10\n",
    " \n",
    "tol_it = np.zeros(n_it)\n",
    "alpha_it = np.zeros(n_it)\n",
    "OF_it=np.zeros(n_it)\n",
    "\n",
    "X1 = X_s[0:50,:]\n",
    "X2 = X_s[51:100,:]\n",
    "X3 = X_s[101:150,:]\n",
    "X4 = X_s[151:200,:]\n",
    "X5 = X_s[201:250,:]\n",
    "X6 = X_s[251:300,:]\n",
    "X7 = X_s[301:350,:]\n",
    "X8 = X_s[351:400,:]\n",
    "X9 = X_s[401:450,:]\n",
    "X10 = X_s[451:500,:]\n",
    "X11 = X_s[501:550,:]\n",
    "X12 = X_s[551:600,:]\n",
    "X13 = X_s[601:650,:]\n",
    "X14 = X_s[651:700,:]\n",
    "X15 = X_s[701:750,:]\n",
    "X16 = X_s[751:800,:]\n",
    "X17 = X_s[801:850,:]\n",
    "X18 = X_s[851:900,:]\n",
    "X19 = X_s[901:950,:]\n",
    "X20 = X_s[951:1000,:]\n",
    "\n",
    "Y1 = Y_s[0:50,:]\n",
    "Y2 = Y_s[51:100,:]\n",
    "Y3 = Y_s[101:150,:]\n",
    "Y4 = Y_s[151:200,:]\n",
    "Y5 = Y_s[201:250,:]\n",
    "Y6 = Y_s[251:300,:]\n",
    "Y7 = Y_s[301:350,:]\n",
    "Y8 = Y_s[351:400,:]\n",
    "Y9 = Y_s[401:450,:]\n",
    "Y10 = Y_s[451:500,:]\n",
    "Y11 = Y_s[501:550,:]\n",
    "Y12 = Y_s[551:600,:]\n",
    "Y13 = Y_s[601:650,:]\n",
    "Y14 = Y_s[651:700,:]\n",
    "Y15 = Y_s[701:750,:]\n",
    "Y16 = Y_s[751:800,:]\n",
    "Y17 = Y_s[801:850,:]\n",
    "Y18 = Y_s[851:900,:]\n",
    "Y19 = Y_s[901:950,:]\n",
    "Y20 = Y_s[951:1000,:]\n",
    "\n",
    "h = len(X1)\n",
    "beta_momentum = np.zeros(b)\n",
    "\n",
    "P = np.concatenate([X1, X2, X3, X4, X5, X6, X7, X8, X9, X10, X11, X12, X13, X14, X15, X16, X17, X18, X19, X20], axis=0)\n",
    "Q = np.concatenate([Y1, Y2, Y3, Y4, Y5, Y6, Y7, Y8, Y9, Y10, Y11, Y12, Y13, Y14, Y15, Y16, Y17, Y18, Y19, Y20], axis=0)\n",
    "\n",
    "start = time.process_time()\n",
    "\n",
    "i = 0\n",
    "    \n",
    "while (i <= n_it-2) and (tol > epsilon):\n",
    "    i=i+1\n",
    "    grad = ridge_reg_der(beta_momentum,P,Q, rho) # Gradient vector\n",
    "   # grad[i] = grad[i] + eta*grad[i-1]\n",
    "    ddirect = -grad/b # Descent direction \n",
    "    beta_momentum = beta_momentum + alpha*ddirect\n",
    "    grad[i] = grad[i] + eta*grad[i-1]\n",
    "\n",
    "    OF_it[i] = ridge_reg(beta_momentum, P, Q, rho)\n",
    "    tol = np.linalg.norm(grad,ord=2)\n",
    "    tol_it[i] = tol\n",
    "    alpha_it[i] = alpha\n",
    "    \n",
    "total_time = (time.process_time() - start)\n",
    "\n",
    "print('Elapsed computational time = %8.5f' %(total_time))\n",
    "print('\\nNumber of iterations = %5.0f' %i)\n",
    "print('Objective function   = %11.5f' %OF_it[i])\n",
    "print('Optimality tolerance = %11.5f' %tol)\n",
    "print('\\nValues of the least squares coefficients with the mini-batch method with momentum:')\n",
    "print('beta %-9s %7.3f' %('intercept',beta_momentum[0]))\n",
    "print('beta %-9s %7.3f' %('1',beta_momentum[1]))\n",
    "print('beta %-9s %7.3f' %('2',beta_momentum[2]))\n",
    "print('beta %-9s %7.3f' %('3',beta_momentum[3]))\n",
    "print('beta %-9s %7.3f' %('4',beta_momentum[4]))\n",
    "print('beta %-9s %7.3f' %('5',beta_momentum[5]))\n",
    "print('beta %-9s %7.3f' %('6',beta_momentum[6]))\n",
    "\n",
    "beta_error = np.linalg.norm(np.transpose(beta_exact)-beta_momentum,ord=2)/np.linalg.norm(beta_momentum,ord=2)\n",
    "print('\\nBeta coefficient error = %10.5f' %beta_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions\n",
    "From the results derived previously, it can be infered that by using unconstrained methods, the errors are considerably smaller than the ones regarding constrained methods (errors of about 0.04), except from the last method, the mini-batch with momentum. This can be due to the number of samples in which the dataset has been divided, as it does not seem to be the optimal one. Nevertheless, for instance, no error is shown with the Newton method, showing perfect accuracy (good accuracy was expected), and it can be concluded that the goal was fulfilled, since the errors were dropped to negligible values starting with constrained methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
