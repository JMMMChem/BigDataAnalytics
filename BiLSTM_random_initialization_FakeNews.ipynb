{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BiLSTM random initialization FakeNews.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3y79AvZ35XEQ"
      },
      "source": [
        "## **BiLSTM with random initilization**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLKHyLAy5fpf"
      },
      "source": [
        "**Load the required dependencies and Keras**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tUTPq8fDnY1K"
      },
      "source": [
        "!wget --quiet https://raw.githubusercontent.com/tensorflow/models/master/official/nlp/bert/tokenization.py\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QgYd2xbPneq9",
        "outputId": "3c437bb1-4988-463f-8a76-fa6f641e7777"
      },
      "source": [
        "!pip install sentencepiece\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n",
            "\r\u001b[K     |▎                               | 10kB 21.1MB/s eta 0:00:01\r\u001b[K     |▌                               | 20kB 17.1MB/s eta 0:00:01\r\u001b[K     |▉                               | 30kB 14.0MB/s eta 0:00:01\r\u001b[K     |█                               | 40kB 13.0MB/s eta 0:00:01\r\u001b[K     |█▍                              | 51kB 8.5MB/s eta 0:00:01\r\u001b[K     |█▋                              | 61kB 8.7MB/s eta 0:00:01\r\u001b[K     |██                              | 71kB 9.0MB/s eta 0:00:01\r\u001b[K     |██▏                             | 81kB 10.1MB/s eta 0:00:01\r\u001b[K     |██▌                             | 92kB 9.1MB/s eta 0:00:01\r\u001b[K     |██▊                             | 102kB 8.1MB/s eta 0:00:01\r\u001b[K     |███                             | 112kB 8.1MB/s eta 0:00:01\r\u001b[K     |███▎                            | 122kB 8.1MB/s eta 0:00:01\r\u001b[K     |███▌                            | 133kB 8.1MB/s eta 0:00:01\r\u001b[K     |███▉                            | 143kB 8.1MB/s eta 0:00:01\r\u001b[K     |████                            | 153kB 8.1MB/s eta 0:00:01\r\u001b[K     |████▍                           | 163kB 8.1MB/s eta 0:00:01\r\u001b[K     |████▋                           | 174kB 8.1MB/s eta 0:00:01\r\u001b[K     |█████                           | 184kB 8.1MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 194kB 8.1MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 204kB 8.1MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 215kB 8.1MB/s eta 0:00:01\r\u001b[K     |██████                          | 225kB 8.1MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 235kB 8.1MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 245kB 8.1MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 256kB 8.1MB/s eta 0:00:01\r\u001b[K     |███████                         | 266kB 8.1MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 276kB 8.1MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 286kB 8.1MB/s eta 0:00:01\r\u001b[K     |████████                        | 296kB 8.1MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 307kB 8.1MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 317kB 8.1MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 327kB 8.1MB/s eta 0:00:01\r\u001b[K     |█████████                       | 337kB 8.1MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 348kB 8.1MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 358kB 8.1MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 368kB 8.1MB/s eta 0:00:01\r\u001b[K     |██████████                      | 378kB 8.1MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 389kB 8.1MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 399kB 8.1MB/s eta 0:00:01\r\u001b[K     |███████████                     | 409kB 8.1MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 419kB 8.1MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 430kB 8.1MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 440kB 8.1MB/s eta 0:00:01\r\u001b[K     |████████████                    | 450kB 8.1MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 460kB 8.1MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 471kB 8.1MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 481kB 8.1MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 491kB 8.1MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 501kB 8.1MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 512kB 8.1MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 522kB 8.1MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 532kB 8.1MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 542kB 8.1MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 552kB 8.1MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 563kB 8.1MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 573kB 8.1MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 583kB 8.1MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 593kB 8.1MB/s eta 0:00:01\r\u001b[K     |████████████████                | 604kB 8.1MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 614kB 8.1MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 624kB 8.1MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 634kB 8.1MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 645kB 8.1MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 655kB 8.1MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 665kB 8.1MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 675kB 8.1MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 686kB 8.1MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 696kB 8.1MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 706kB 8.1MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 716kB 8.1MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 727kB 8.1MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 737kB 8.1MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 747kB 8.1MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 757kB 8.1MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 768kB 8.1MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 778kB 8.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 788kB 8.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 798kB 8.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 808kB 8.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 819kB 8.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 829kB 8.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 839kB 8.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 849kB 8.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 860kB 8.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 870kB 8.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 880kB 8.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 890kB 8.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 901kB 8.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 911kB 8.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 921kB 8.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 931kB 8.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 942kB 8.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 952kB 8.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 962kB 8.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 972kB 8.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 983kB 8.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 993kB 8.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 1.0MB 8.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.0MB 8.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 1.0MB 8.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 1.0MB 8.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 1.0MB 8.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.1MB 8.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 1.1MB 8.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 1.1MB 8.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 1.1MB 8.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 1.1MB 8.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 1.1MB 8.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.1MB 8.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.1MB 8.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 1.1MB 8.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 1.1MB 8.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 1.2MB 8.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.2MB 8.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 1.2MB 8.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 1.2MB 8.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 1.2MB 8.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.2MB 8.1MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.95\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hUnpUvX9ohkc",
        "outputId": "60dbfc42-a063-4ae6-9d95-fe366bb4978c"
      },
      "source": [
        "pip install --upgrade keras"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: keras in /usr/local/lib/python3.7/dist-packages (2.4.3)\n",
            "Requirement already satisfied, skipping upgrade: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from keras) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: h5py in /usr/local/lib/python3.7/dist-packages (from keras) (2.10.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras) (1.19.5)\n",
            "Requirement already satisfied, skipping upgrade: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras) (3.13)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.7/dist-packages (from h5py->keras) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o5g7B1Wxnis8"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Dense, Input, Embedding, Dropout, Activation, Flatten, Bidirectional, GlobalMaxPool1D\n",
        "from keras.models import Model, Sequential\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "from keras.layers import LSTM\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PRokh2MdnwJp",
        "outputId": "3960f3d3-6c08-4fdf-be10-761c3f8ef26c"
      },
      "source": [
        "# Load the Drive helper and mount\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PAlxTFIm5tHQ"
      },
      "source": [
        "**Load the csv files**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ruL2MpWRnwhR"
      },
      "source": [
        "cod_train = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/fake-news/train\")\n",
        "cod_train2, test= train_test_split(cod_train, random_state=0, test_size=0.2)\n",
        "train, val =  train_test_split(cod_train2, random_state = 0,test_size=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I1PlJQ8y5veV"
      },
      "source": [
        "**Tokenizer**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MIgh99Gbn1bi",
        "outputId": "bf531392-7914-4466-813e-409c81679d27"
      },
      "source": [
        "max_features = 100000 # max num words\n",
        "maxlen = 250 \n",
        "embedding_size = 200\n",
        "\n",
        "# create the tokenizer with the maximum number of words to keep, \n",
        "# based on word frequency. \n",
        "# Only the most common num_words-1 words will be kept.\n",
        "tokenizer = Tokenizer(num_words=max_features, oov_token = True)\n",
        "\n",
        "train['text']=train['text'].astype(str)\n",
        "test['text']=test['text'].astype(str)\n",
        "val['text']=val['text'].astype(str)\n",
        "\n",
        "# fit the tokenizer on the headlines\n",
        "tokenizer.fit_on_texts(list(train['text']))\n",
        "\n",
        "# Transforms each text in texts to a sequence of integers.\n",
        "train_X = tokenizer.texts_to_sequences(train['text'])\n",
        "test_X = tokenizer.texts_to_sequences(test['text'])\n",
        "val_X = tokenizer.texts_to_sequences(val['text'])\n",
        "\n",
        "# transforms a list of num_samples sequences (lists of integers)\n",
        "# into a 2D Numpy array of shape (num_samples, num_timesteps).\n",
        "train_X = pad_sequences(train_X, maxlen = maxlen)\n",
        "test_X = pad_sequences(test_X, maxlen = maxlen)\n",
        "val_X = pad_sequences(val_X, maxlen = maxlen)\n",
        "\n",
        "train_y = train['label']\n",
        "test_y = test['label']\n",
        "val_y = val['label']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  # Remove the CWD from sys.path while we load stuff.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  # This is added back by InteractiveShellApp.init_path()\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  if sys.path[0] == '':\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nNF6Nxjn5ziH"
      },
      "source": [
        "**The BiLSTM construction**\n",
        "\n",
        "- Activation Function: ReLU has been used as the activation function. It is a non-linear activation function, helping complex relationships in the data to be captured by the model.\n",
        "\n",
        "- Optimizer: Adam optimizer, an adaptive learning rate optimizer.\n",
        "\n",
        "- Loss function: The network will be trained to output a probability over the 2 classes using Sigmoid Loss."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOWkKitGn4NL"
      },
      "source": [
        "sequence_length = train_X.shape[1]\n",
        "\n",
        "model = Sequential()\n",
        "#model.add(Embedding(max_features, embedding_size, weights = [embedding_matrix]))\n",
        "model.add(Embedding(max_features, embedding_size, input_length = sequence_length))\n",
        "model.add(Bidirectional(LSTM(128, return_sequences = True)))\n",
        "model.add(GlobalMaxPool1D())\n",
        "model.add(Dense(40, activation=\"relu\"))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(20, activation=\"relu\"))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(1, activation=\"sigmoid\"))\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy',keras.metrics.Precision(), keras.metrics.Recall(), keras.metrics.TruePositives()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XdyPEs926so2"
      },
      "source": [
        "**Save the best model, early stopping and fit the model**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4gaSF10Ln6hT",
        "outputId": "211e9d6a-98fa-4675-97ef-1657f9898354"
      },
      "source": [
        "\n",
        "# Save the model after every epoch.\n",
        "saveBestModel = keras.callbacks.ModelCheckpoint('/content/drive/My Drive/TFMColab/best_model.hdf5', monitor='val_acc', verbose=0, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
        "# Stop training when a monitored quantity has stopped improving.\n",
        "earlyStopping = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\n",
        "batch_size = 100\n",
        "epochs = 25\n",
        "model.fit(train_X, train_y, batch_size=batch_size, epochs=epochs, validation_data=(val_X, val_y), callbacks=[saveBestModel, earlyStopping])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "Epoch 1/25\n",
            "134/134 [==============================] - 70s 270ms/step - loss: 0.6139 - accuracy: 0.6700 - precision: 0.6967 - recall: 0.5827 - true_positives: 2111.7185 - val_loss: 0.3945 - val_accuracy: 0.8543 - val_precision: 0.9087 - val_recall: 0.7881 - val_true_positives: 1313.0000\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 2/25\n",
            "134/134 [==============================] - 35s 259ms/step - loss: 0.3254 - accuracy: 0.8790 - precision: 0.8909 - recall: 0.8630 - true_positives: 2987.9778 - val_loss: 0.1758 - val_accuracy: 0.9318 - val_precision: 0.9161 - val_recall: 0.9508 - val_true_positives: 1584.0000\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 3/25\n",
            "134/134 [==============================] - 35s 258ms/step - loss: 0.1244 - accuracy: 0.9647 - precision: 0.9638 - recall: 0.9652 - true_positives: 3264.5630 - val_loss: 0.1500 - val_accuracy: 0.9420 - val_precision: 0.9250 - val_recall: 0.9622 - val_true_positives: 1603.0000\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 4/25\n",
            "134/134 [==============================] - 35s 260ms/step - loss: 0.0438 - accuracy: 0.9893 - precision: 0.9871 - recall: 0.9917 - true_positives: 3384.2667 - val_loss: 0.2197 - val_accuracy: 0.9444 - val_precision: 0.9732 - val_recall: 0.9142 - val_true_positives: 1523.0000\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 5/25\n",
            "134/134 [==============================] - 35s 261ms/step - loss: 0.0273 - accuracy: 0.9944 - precision: 0.9939 - recall: 0.9948 - true_positives: 3370.0074 - val_loss: 0.3160 - val_accuracy: 0.9408 - val_precision: 0.9171 - val_recall: 0.9694 - val_true_positives: 1615.0000\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 6/25\n",
            "134/134 [==============================] - 35s 259ms/step - loss: 0.0128 - accuracy: 0.9979 - precision: 0.9972 - recall: 0.9986 - true_positives: 3367.3852 - val_loss: 0.2485 - val_accuracy: 0.9480 - val_precision: 0.9483 - val_recall: 0.9478 - val_true_positives: 1579.0000\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f8ac072e990>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "corWCTEp63TH"
      },
      "source": [
        "**Load the metrics and show them**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RD0CVUgCoA07",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "992d31ed-b555-476e-f07c-bab7fdd8dc96"
      },
      "source": [
        "loss, accuracy, precision, recall, true_positives = model.evaluate(test_X, test_y, batch_size=batch_size)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "42/42 [==============================] - 1s 22ms/step - loss: 0.2252 - accuracy: 0.9517 - precision: 0.9497 - recall: 0.9555 - true_positives: 2020.0000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OfIfDFBtoDQr"
      },
      "source": [
        "mult_pr=precision*recall\n",
        "sum_pr=precision+recall\n",
        "div=mult_pr/sum_pr\n",
        "f1_score=2*div"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HJShsa4doFkE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b09836b-8aca-48ac-a63b-60036f63471e"
      },
      "source": [
        "print('Loss:',loss)\n",
        "print('Accuracy:',accuracy)\n",
        "print('Precision:',precision)\n",
        "print('Recall:',recall)\n",
        "print('f1 score:',f1_score)\n",
        "print('True positives:',true_positives)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss: 0.22517189383506775\n",
            "Accuracy: 0.9516826868057251\n",
            "Precision: 0.9496943950653076\n",
            "Recall: 0.9555345177650452\n",
            "f1 score: 0.9526055055135503\n",
            "True positives: 2020.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sOLdyxLsoMN5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3659f331-b4b7-472d-e2e7-235dfbdf71cf"
      },
      "source": [
        "pred_y = model.predict_classes(test_X, batch_size=batch_size)\n",
        "confusion_matrix(test_y, pred_y)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1939,  107],\n",
              "       [  94, 2020]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vh6hYNop685z"
      },
      "source": [
        "**Finally, extract False Positives and False Negatives to csv files**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t0rs6J6WOEt9"
      },
      "source": [
        "def getFP_FN_TP_lists(test_X, test_y, pred_y):\n",
        "    FP_text = []\n",
        "    FP_index = []\n",
        "    FN_text = []\n",
        "    FN_index = []\n",
        "    TP_text = []\n",
        "    TP_index = []\n",
        "    for i in range(len(test_y)):\n",
        "        if(pred_y[i]==1 and test_y[test_y.index[i]]==0):\n",
        "            FP_text.append(test['text'][test_y.index[i]])\n",
        "            FP_index.append(test_y.index[i])\n",
        "        elif(pred_y[i]==0 and test_y[test_y.index[i]]==1):\n",
        "            FN_text.append(test['text'][test_y.index[i]])\n",
        "            FN_index.append(test_y.index[i])\n",
        "        elif(pred_y[i]==1 and test_y[test_y.index[i]]==1):\n",
        "            TP_text.append(test['text'][test_y.index[i]])\n",
        "            TP_index.append(test_y.index[i])        \n",
        "            \n",
        "    return FP_text,FP_index,FN_text,FN_index,TP_text,TP_index\n",
        "\n",
        "def getFP_FN_TP(test_X, test_y, pred_y):\n",
        "    FP_text,FP_index,FN_text,FN_index,TP_text,TP_index = getFP_FN_TP_lists(test_X, test_y, pred_y)\n",
        "    d_FP = {'FP_text':FP_text,'FP_index':FP_index}\n",
        "    df_FP = pd.DataFrame(d_FP)\n",
        "    d_FN = {'FN_text':FN_text,'FN_index':FN_index}\n",
        "    df_FN = pd.DataFrame(d_FN)\n",
        "    d_TP =  {'TP_text':TP_text,'TP_index':TP_index}\n",
        "    df_TP = pd.DataFrame(d_TP)\n",
        "    \n",
        "    return df_FP,df_FN,df_TP\n",
        "\n",
        "df_FP,df_FN, df_TP = getFP_FN_TP(test_X, test_y, pred_y)\n",
        "df_FP.to_csv('FP_BiLSTMrandom.csv', index=True)\n",
        "df_FN.to_csv('FN_BiLSTMrandom.csv', index=True)\n",
        "df_TP.to_csv('TP_BiLSTMrandom.csv', index=True)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}