{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BiLSTM GloVe FakeNews.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4o-8TqnfcBrt"
      },
      "source": [
        "##**Fake News: LSTM Analysis**\n",
        "\n",
        "Using a dataset containing tweets and their classification as fake news or true news, in this notebook the LSTM is going to be implemented and illustrated"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dv0yeuf0bwyn"
      },
      "source": [
        "**What is done in the Notebook:**\n",
        "\n",
        "· Load the required dependencies\n",
        "\n",
        "· Define helper functions\n",
        "\n",
        "· Load BERT from the Tensorflow Hub\n",
        "\n",
        "· Load CSV files containing data\n",
        "\n",
        "· Tokenization\n",
        "\n",
        "· Build LSTM Model\n",
        "\n",
        "· Save the best model and early stopping\n",
        "\n",
        "· Fit the model\n",
        "\n",
        "· Evaluate model results with test data\n",
        "\n",
        "· Extract False Positives and False Negatives"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-AKF-2JcJuK"
      },
      "source": [
        "**Load the required dependencies**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IYarGmYY2ZPQ"
      },
      "source": [
        "!wget --quiet https://raw.githubusercontent.com/tensorflow/models/master/official/nlp/bert/tokenization.py\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v-4ceywn2aa8",
        "outputId": "c27befcd-c45a-40f6-c143-132b05b9aa06"
      },
      "source": [
        "!pip install sentencepiece\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 6.7MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.95\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oyqh7oXe24CZ",
        "outputId": "25b65c91-fa38-4b4f-a821-2a3999805ac2"
      },
      "source": [
        "pip install --upgrade keras"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: keras in /usr/local/lib/python3.7/dist-packages (2.4.3)\n",
            "Requirement already satisfied, skipping upgrade: h5py in /usr/local/lib/python3.7/dist-packages (from keras) (2.10.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras) (1.19.5)\n",
            "Requirement already satisfied, skipping upgrade: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from keras) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras) (3.13)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.7/dist-packages (from h5py->keras) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tKWoKLs0boiY"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Dense, Input, Embedding, Dropout, Activation, Flatten, Bidirectional, GlobalMaxPool1D\n",
        "from keras.models import Model, Sequential\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "from keras.layers import LSTM\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvFL0Zs7cUrq"
      },
      "source": [
        "**Load CSV files containing data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79236piecSlq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "311bbd9a-eefe-409c-e12b-cee67e431ff1"
      },
      "source": [
        "# Load the Drive helper and mount\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZLDqOTRccZw7"
      },
      "source": [
        "cod_train = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/fake-news/train\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tXWcU67u6OI1"
      },
      "source": [
        "cod_train2, test= train_test_split(cod_train, random_state=0, test_size=0.2)\n",
        "train, val =  train_test_split(cod_train2, random_state = 0,test_size=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bwStYFpR9nJL",
        "outputId": "54189093-877c-434c-db53-599660873c74"
      },
      "source": [
        "train['text'].head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5043     RIO DE JANEIRO  —   Brazil’s president is faci...\n",
              "6612     Black Activist Blasts Hillary Clinton Over Wik...\n",
              "5131     (Table 9) Prec Metals= Mostly Silver \\nJP Morg...\n",
              "18362    Islamists around the Arab world celebrated Tur...\n",
              "9430     14 Shares\\n12 0 0 2 (Taksim Square - Gezi Park...\n",
              "Name: text, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WrIYdNttcYR6"
      },
      "source": [
        "**Tokenization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fAEYWjy7chD0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8548b978-5a1e-47df-96f0-c0e6c1c4a0df"
      },
      "source": [
        "max_features = 100000 # max num words\n",
        "maxlen = 250 \n",
        "embedding_size = 200\n",
        "\n",
        "# create the tokenizer with the maximum number of words to keep, \n",
        "# based on word frequency. \n",
        "# Only the most common num_words-1 words will be kept.\n",
        "tokenizer = Tokenizer(num_words=max_features, oov_token = True)\n",
        "\n",
        "train['text']=train['text'].astype(str)\n",
        "test['text']=test['text'].astype(str)\n",
        "val['text']=val['text'].astype(str)\n",
        "\n",
        "# fit the tokenizer on the headlines\n",
        "tokenizer.fit_on_texts(list(train['text']))\n",
        "\n",
        "# Transforms each text in texts to a sequence of integers.\n",
        "train_X = tokenizer.texts_to_sequences(train['text'])\n",
        "test_X = tokenizer.texts_to_sequences(test['text'])\n",
        "val_X = tokenizer.texts_to_sequences(val['text'])\n",
        "\n",
        "# transforms a list of num_samples sequences (lists of integers)\n",
        "# into a 2D Numpy array of shape (num_samples, num_timesteps).\n",
        "train_X = pad_sequences(train_X, maxlen = maxlen)\n",
        "test_X = pad_sequences(test_X, maxlen = maxlen)\n",
        "val_X = pad_sequences(val_X, maxlen = maxlen)\n",
        "\n",
        "train_y = train['label']\n",
        "test_y = test['label']\n",
        "val_y = val['label']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  # Remove the CWD from sys.path while we load stuff.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  # This is added back by InteractiveShellApp.init_path()\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  if sys.path[0] == '':\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BhiN1yJb7n8I"
      },
      "source": [
        "\n",
        "Load glove embedding set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S-OdEFbO7mj5",
        "outputId": "a3ad18a6-a60d-4aea-afcc-b93ae3e3cb9e"
      },
      "source": [
        "\n",
        "# load embeddings\n",
        "EMBEDDING_FILE = '/content/drive/MyDrive/Colab Notebooks/fake-news/glove.6B.200d.txt'\n",
        "\n",
        "def get_coefs(word,*arr): \n",
        "    return word, np.asarray(arr, dtype='float32')\n",
        "\n",
        "# first, build index mapping words in the embeddings set\n",
        "# to their embedding vector\n",
        "embeddings_index = {}\n",
        "with open(EMBEDDING_FILE, encoding=\"utf8\") as f:\n",
        "    for line in f:\n",
        "        word, coefs = get_coefs(*line.split(\" \"))\n",
        "        embeddings_index[word] = coefs\n",
        "            \n",
        "all_embs = np.stack(embeddings_index.values())\n",
        "emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
        "embed_size = all_embs.shape[1]\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "nb_words = min(max_features, len(word_index))\n",
        "\n",
        "# Random embedding vector for unknown words.\n",
        "embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embedding_size))\n",
        "# prepare embedding matrix\n",
        "for word, i in word_index.items():\n",
        "    if i >= max_features: \n",
        "        continue\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None: \n",
        "        # words not found in embedding index will be random\n",
        "        embedding_matrix[i] = embedding_vector"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2822: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
            "  if self.run_code(code, result):\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvIGMsNU7vOO"
      },
      "source": [
        "\n",
        "**Build BiLSTM Model**\n",
        "\n",
        "- Activation Function: ReLU has been used as the activation function. It is a non-linear activation function, helping complex relationships in the data to be captured by the model.\n",
        "\n",
        "- Optimizer: Adam optimizer, an adaptive learning rate optimizer.\n",
        "\n",
        "- Loss function: The network will be trained to output a probability over the 2 classes using Sigmoid Loss."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wxHgozOf7rq7"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(max_features, embedding_size, weights = [embedding_matrix]))\n",
        "model.add(Bidirectional(LSTM(128, return_sequences = True)))\n",
        "model.add(GlobalMaxPool1D())\n",
        "model.add(Dense(40, activation=\"relu\"))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(20, activation=\"relu\"))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(1, activation=\"sigmoid\"))\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy',keras.metrics.Precision(), keras.metrics.Recall(), keras.metrics.TruePositives()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "quBLXN6W74OA"
      },
      "source": [
        "\n",
        "**Save the best model and early stopping**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-gd6X9rK7zxF",
        "outputId": "913fd5d1-c86b-4d95-ed4a-b9bc030824ce"
      },
      "source": [
        "\n",
        "# Save the model after every epoch.\n",
        "saveBestModel = keras.callbacks.ModelCheckpoint('/content/drive/My Drive/TFMColab/best_model.hdf5', monitor='val_acc', verbose=0, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
        "# Stop training when a monitored quantity has stopped improving.\n",
        "earlyStopping = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k57nNDQ48BJN"
      },
      "source": [
        "**Fit the model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iBuhKFDz79PA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cba1f817-7519-4404-ba39-e2f90f677836"
      },
      "source": [
        "batch_size = 100\n",
        "epochs = 25\n",
        "model.fit(train_X, train_y, batch_size=batch_size, epochs=epochs, validation_data=(val_X, val_y), callbacks=[saveBestModel, earlyStopping])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "134/134 [==============================] - 293s 2s/step - loss: 0.6072 - accuracy: 0.6478 - precision: 0.6598 - recall: 0.6070 - true_positives: 2181.3852 - val_loss: 0.2484 - val_accuracy: 0.8960 - val_precision: 0.9342 - val_recall: 0.8523 - val_true_positives: 1420.0000\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 2/25\n",
            "134/134 [==============================] - 292s 2s/step - loss: 0.2211 - accuracy: 0.9197 - precision: 0.9149 - recall: 0.9268 - true_positives: 3156.1556 - val_loss: 0.1435 - val_accuracy: 0.9390 - val_precision: 0.9280 - val_recall: 0.9520 - val_true_positives: 1586.0000\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 3/25\n",
            "134/134 [==============================] - 288s 2s/step - loss: 0.1056 - accuracy: 0.9709 - precision: 0.9723 - recall: 0.9694 - true_positives: 3275.7481 - val_loss: 0.1433 - val_accuracy: 0.9450 - val_precision: 0.9661 - val_recall: 0.9226 - val_true_positives: 1537.0000\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 4/25\n",
            "134/134 [==============================] - 294s 2s/step - loss: 0.0453 - accuracy: 0.9859 - precision: 0.9874 - recall: 0.9844 - true_positives: 3350.3407 - val_loss: 0.1434 - val_accuracy: 0.9507 - val_precision: 0.9492 - val_recall: 0.9526 - val_true_positives: 1587.0000\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 5/25\n",
            "134/134 [==============================] - 290s 2s/step - loss: 0.0219 - accuracy: 0.9960 - precision: 0.9962 - recall: 0.9958 - true_positives: 3361.2815 - val_loss: 0.2179 - val_accuracy: 0.9543 - val_precision: 0.9582 - val_recall: 0.9502 - val_true_positives: 1583.0000\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 6/25\n",
            "134/134 [==============================] - 291s 2s/step - loss: 0.0105 - accuracy: 0.9980 - precision: 0.9981 - recall: 0.9978 - true_positives: 3390.6074 - val_loss: 0.2300 - val_accuracy: 0.9519 - val_precision: 0.9393 - val_recall: 0.9664 - val_true_positives: 1610.0000\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f719ae74350>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yuOaxScG8G6s"
      },
      "source": [
        "**Evaluate model results with test data**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NeCx_Ow18HIL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c18f18f7-aa72-4ac0-cd34-67917aca3c83"
      },
      "source": [
        "model.metrics_names\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['loss', 'accuracy', 'precision', 'recall', 'true_positives']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YMvilD9F8HwO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b61ec60b-aafa-4c28-97d8-5530b70cf8fe"
      },
      "source": [
        "loss, accuracy, precision, recall, true_positives = model.evaluate(test_X, test_y, batch_size=batch_size)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "42/42 [==============================] - 23s 558ms/step - loss: 0.2124 - accuracy: 0.9524 - precision: 0.9407 - recall: 0.9674 - true_positives: 2045.0000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dutTaN-Z8JO4"
      },
      "source": [
        "mult_pr=precision*recall\n",
        "sum_pr=precision+recall\n",
        "div=mult_pr/sum_pr\n",
        "f1_score=2*div"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eTYnnenI8Rze"
      },
      "source": [
        "**Loss, Accuracy, Precision, Recall and F1**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mxe2I18d8RUw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "053aebbb-ef87-41ec-c509-90b37b7975f3"
      },
      "source": [
        "print('Loss:',loss)\n",
        "print('Accuracy:',accuracy)\n",
        "print('Precision:',precision)\n",
        "print('Recall:',recall)\n",
        "print('f1 score:',f1_score)\n",
        "print('True positives:',true_positives)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss: 0.2124175727367401\n",
            "Accuracy: 0.9524038434028625\n",
            "Precision: 0.9406623840332031\n",
            "Recall: 0.9673604369163513\n",
            "f1 score: 0.9538246239175293\n",
            "True positives: 2045.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GB41gZV28YEW"
      },
      "source": [
        "**Extract False Positives and False Negatives**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SkaAIJei8Zso",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e028c482-da08-4ca8-bcca-b796f27fe1d6"
      },
      "source": [
        "pred_y = model.predict_classes(test_X, batch_size=batch_size)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_GepsDmb8bPw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f259087e-5444-4a19-b99e-9b6a8974776a"
      },
      "source": [
        "confusion_matrix(test_y, pred_y)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1917,  129],\n",
              "       [  69, 2045]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iPv_LAis_aJW"
      },
      "source": [
        "def getFP_FN_TP_lists(test_X, test_y, pred_y):\n",
        "    FP_text = []\n",
        "    FP_index = []\n",
        "    FN_text = []\n",
        "    FN_index = []\n",
        "    TP_text = []\n",
        "    TP_index = []\n",
        "    for i in range(len(test_y)):\n",
        "        if(pred_y[i]==1 and test_y[test_y.index[i]]==0):\n",
        "            FP_text.append(test['text'][test_y.index[i]])\n",
        "            FP_index.append(test_y.index[i])\n",
        "        elif(pred_y[i]==0 and test_y[test_y.index[i]]==1):\n",
        "            FN_text.append(test['text'][test_y.index[i]])\n",
        "            FN_index.append(test_y.index[i])\n",
        "        elif(pred_y[i]==1 and test_y[test_y.index[i]]==1):\n",
        "            TP_text.append(test['text'][test_y.index[i]])\n",
        "            TP_index.append(test_y.index[i])        \n",
        "            \n",
        "    return FP_text,FP_index,FN_text,FN_index,TP_text,TP_index\n",
        "\n",
        "def getFP_FN_TP(test_X, test_y, pred_y):\n",
        "    FP_text,FP_index,FN_text,FN_index,TP_text,TP_index = getFP_FN_TP_lists(test_X, test_y, pred_y)\n",
        "    d_FP = {'FP_text':FP_text,'FP_index':FP_index}\n",
        "    df_FP = pd.DataFrame(d_FP)\n",
        "    d_FN = {'FN_text':FN_text,'FN_index':FN_index}\n",
        "    df_FN = pd.DataFrame(d_FN)\n",
        "    d_TP =  {'TP_text':TP_text,'TP_index':TP_index}\n",
        "    df_TP = pd.DataFrame(d_TP)\n",
        "    \n",
        "    return df_FP,df_FN,df_TP\n",
        "\n",
        "df_FP,df_FN, df_TP = getFP_FN_TP(test_X, test_y, pred_y)\n",
        "df_FP.to_csv('FP_BiLSTMGlove.csv', index=True)\n",
        "df_FN.to_csv('FN_BiLSTMGlove.csv', index=True)\n",
        "df_TP.to_csv('TP_BiLSTMGlove.csv', index=True)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
