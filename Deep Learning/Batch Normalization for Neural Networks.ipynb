{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "JoseMaria_Lab_3_Part_2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "360711691d4e4aaf942b0a8273e5cab9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_06d05010c743477fadeb4329a78b2d0b",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_359e98ccd0a44c63958b07f060aafb7c",
              "IPY_MODEL_19cb2cb72cbb48b7bc0a2dc3acd834a9"
            ]
          }
        },
        "06d05010c743477fadeb4329a78b2d0b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "359e98ccd0a44c63958b07f060aafb7c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_04284fe814424b0e9b40f6599a649d20",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 170498071,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 170498071,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c415d16c8275416cb1b7fdf8dc5d2052"
          }
        },
        "19cb2cb72cbb48b7bc0a2dc3acd834a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_d9e8bd121ad0477c91d87dd8a045907e",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 170499072/? [00:10&lt;00:00, 15591742.57it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_41060d57b0fd44eab02949f1bd8a4304"
          }
        },
        "04284fe814424b0e9b40f6599a649d20": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c415d16c8275416cb1b7fdf8dc5d2052": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d9e8bd121ad0477c91d87dd8a045907e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "41060d57b0fd44eab02949f1bd8a4304": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "layB9pEvUwqf"
      },
      "source": [
        "# Lab 3 Part 2: Batch Normalization \n",
        "\n",
        "José María Martínez Marín 100443343\n",
        "\n",
        "Azamat Ziiadinov 100460540\n",
        "\n",
        "------------------------------------------------------\n",
        "*Deep Learning. Master in Big Data Analytics*\n",
        "\n",
        "*Pablo M. Olmos pamartin@ing.uc3m.es*\n",
        "\n",
        "------------------------------------------------------\n",
        "\n",
        "Batch normalization was introduced in Sergey Ioffe's and Christian Szegedy's 2015 paper [Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](https://arxiv.org/pdf/1502.03167.pdf). The idea is that, instead of just normalizing the inputs to the network, we normalize the inputs to _layers within_ the network. \n",
        "\n",
        "> It's called **batch** normalization because during training, we normalize each layer's inputs by using the mean and variance of the values in the current *batch*.\n",
        "\n",
        "We will first analyze the effect of Batch Normalization (BN) in a simple NN with dense layers. Then you will be able to incorportate BN into the CNN that you designed in the first part of Lab 3. \n",
        "\n",
        "Note: a big part of the following material is a personal wrap-up of [Facebook's Deep Learning Course in Udacity](https://www.udacity.com/course/deep-learning-pytorch--ud188). So all credit goes for them!!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "siyfhjMsUwqh"
      },
      "source": [
        "## Batch Normalization in PyTorch<a id=\"implementation_1\"></a>\n",
        "\n",
        "This section of the notebook shows you one way to add batch normalization to a neural network built in PyTorch. \n",
        "\n",
        "The following cells import the packages we need in the notebook and load the MNIST dataset to use in our experiments."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3FN3FRcAUwqi"
      },
      "source": [
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'  #To get figures with high quality!\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "import matplotlib.pyplot as plt\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ePWl9EBu0Yzp",
        "outputId": "9c12343e-8fd8-4aa4-b37e-566c3bec1d06"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')\r\n",
        "\r\n",
        "from torchvision import datasets, transforms\r\n",
        "\r\n",
        "# Define a transform to normalize the data\r\n",
        "transform = transforms.Compose([transforms.ToTensor(),\r\n",
        "                              transforms.Normalize((0.5,), (0.5,)),\r\n",
        "                              ])\r\n",
        "\r\n",
        "sst_home='drive/MyDrive/'\r\n",
        "#modify this path \r\n",
        "path=sst_home+'MNIST_data/MNIST_data'\r\n",
        "\r\n",
        "trainset = datasets.MNIST(path, download=False, train=True, transform=transform)\r\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\r\n",
        "\r\n",
        "testset = datasets.MNIST(path, download=False, train=False, transform=transform)\r\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hlate1DnUwqx"
      },
      "source": [
        "### Neural network classes\n",
        "\n",
        "The following class, `MLP`, allows us to create identical neural networks **with and without batch normalization** to compare. We are defining a simple NN with **two dense layers** for classification; this design choice was made to support the discussion related to batch normalization and not to get the best classification accuracy.\n",
        "\n",
        "Two importants points about BN:\n",
        "\n",
        "- We use PyTorch's [BatchNorm1d](https://pytorch.org/docs/stable/nn.html#batchnorm1d). This is the function you use to operate on linear layer outputs; you'll use [BatchNorm2d](https://pytorch.org/docs/stable/nn.html#batchnorm2d) for 2D outputs like filtered images from convolutional layers. \n",
        "- We add the batch normalization layer **before** calling the activation function.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4OW8ZWM1Uwqz"
      },
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self,dimx,hidden1,hidden2,nlabels,use_batch_norm): #Nlabels will be 10 in our case\n",
        "        \n",
        "        super().__init__()\n",
        "        \n",
        "        # Keep track of whether or not this network uses batch normalization.\n",
        "        self.use_batch_norm = use_batch_norm\n",
        "        \n",
        "        self.output1 = nn.Linear(dimx,hidden1)\n",
        "        \n",
        "        self.output2 = nn.Linear(hidden1,hidden2)        \n",
        "        \n",
        "        self.output3 = nn.Linear(hidden2,nlabels)\n",
        "    \n",
        "        self.relu = nn.ReLU()\n",
        "        \n",
        "        self.logsoftmax = nn.LogSoftmax(dim=1)\n",
        "        \n",
        "        if self.use_batch_norm:\n",
        "\n",
        "            self.batch_norm1 = nn.BatchNorm1d(hidden1)\n",
        "            \n",
        "            self.batch_norm2 = nn.BatchNorm1d(hidden2)\n",
        "            \n",
        "        \n",
        "    def forward(self, x):\n",
        "        # Pass the input tensor through each of our operations\n",
        "        x = self.output1(x)\n",
        "        if self.use_batch_norm:\n",
        "            x = self.batch_norm1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.output2(x)\n",
        "        if self.use_batch_norm:\n",
        "            x = self.batch_norm2(x)        \n",
        "        x = self.relu(x)\n",
        "        x = self.output3(x)\n",
        "        x = self.logsoftmax(x) \n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_DRvyjGMUwq3"
      },
      "source": [
        "> **Exercise:** \n",
        "> \n",
        "> - Create a validation set with the 20% of training set\n",
        "> - Extend the class above to incorporate a training method where both training and validation losses are computed, and a method to evaluate the classification performance on a given set\n",
        "\n",
        "**Note:** As we do with Dropout, for BN we have to call the methods `self.eval()` and `self.train()` in both validation and training. Setting a model to evaluation mode is important for models with batch normalization layers!\n",
        "\n",
        ">* Training mode means that the batch normalization layers will use **batch** statistics to calculate the batch norm. \n",
        "* Evaluation mode, on the other hand, uses the estimated **population** mean and variance from the entire training set, which should give us increased performance on this test data!  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0UAT1IgvUwq4"
      },
      "source": [
        "\r\n",
        "\r\n",
        "import copy\r\n",
        "\r\n",
        "numval = int(len(trainset)*0.8)\r\n",
        "\r\n",
        "validloader = copy.deepcopy(trainloader)  # Creates a copy of the object \r\n",
        "full_trainloader = copy.deepcopy(trainloader)\r\n",
        "\r\n",
        "\r\n",
        "#We take the first  images for training\r\n",
        "trainloader.dataset.data = trainloader.dataset.data[:numval,:,:]\r\n",
        "trainloader.dataset.targets = trainloader.dataset.targets[:numval]\r\n",
        "\r\n",
        "#And the rest for validation\r\n",
        "validloader.dataset.data = validloader.dataset.data[numval:,:,:]\r\n",
        "validloader.dataset.targets = validloader.dataset.targets[numval:]\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ejAtG3mjsST4"
      },
      "source": [
        "class MLP_extended(MLP):\r\n",
        "    \r\n",
        "    def __init__(self,dimx,hidden1,hidden2,nlabels,use_batch_norm,epochs=100,lr=0.001):   \r\n",
        "        super().__init__(dimx,hidden1,hidden2,nlabels,use_batch_norm)  #To initialize MLP!      \r\n",
        "        self.lr = lr #Learning Rate      \r\n",
        "        self.optim = optim.Adam(self.parameters(), self.lr)      \r\n",
        "        self.epochs = epochs       \r\n",
        "        self.criterion = nn.NLLLoss()    \r\n",
        "        self.loss_during_training = []\r\n",
        "        # A list to store the loss evolution along validation    \r\n",
        "        self.valid_loss_during_training = [] \r\n",
        "\r\n",
        "    def valid_loss(self, validloader): \r\n",
        "        if validloader is not None:  \r\n",
        "            with torch.no_grad():\r\n",
        "                self.eval()\r\n",
        "                running_loss = 0.    \r\n",
        "                for (images, labels) in validloader:\r\n",
        "                    out = self.forward(images.view(images.shape[0], -1))\r\n",
        "                    loss = self.criterion(out, labels)\r\n",
        "                    running_loss += loss.item()    \r\n",
        "                self.valid_loss_during_training.append(running_loss/len(validloader))    \r\n",
        "        else:  \r\n",
        "            raise ValueError('validloader must contain data.')        \r\n",
        "\r\n",
        "    # set model back to train mode\r\n",
        "        self.train() \r\n",
        "                  \r\n",
        "    def trainloop(self,trainloader,validloader = None):     \r\n",
        "        # Adam Loop     \r\n",
        "        for e in range(int(self.epochs)):      \r\n",
        "            running_loss = 0.\r\n",
        "\r\n",
        "            for images, labels in trainloader:             \r\n",
        "                self.optim.zero_grad()  #TO RESET GRADIENTS!\r\n",
        "                out = self.forward(images.view(images.shape[0], -1))             \r\n",
        "                loss = self.criterion(out, labels)\r\n",
        "                running_loss += loss.item()\r\n",
        "                loss.backward()\r\n",
        "                self.optim.step()       \r\n",
        "\r\n",
        "            self.loss_during_training.append(running_loss/len(trainloader))\r\n",
        "            if validloader is not None:\r\n",
        "                self.valid_loss(validloader)\r\n",
        "\r\n",
        "            if(e % 1 == 0): # Every 1 epochs              \r\n",
        "                print(\"Training loss after %d iterations: %f\" \r\n",
        "                      %(e,self.loss_during_training[-1]))              \r\n",
        "                print(\"Validation loss after %d epochs: %f\" \r\n",
        "                      %(e,self.valid_loss_during_training[-1]))           \r\n",
        "\r\n",
        "    def accuracy(self, loader):     \r\n",
        "        accuracy = 0\r\n",
        "        # Turn off gradients for validation, saves memory and computations\r\n",
        "        with torch.no_grad():\r\n",
        "          for (images, labels) in loader:\r\n",
        "            logprobs = self.forward(images.view(images.shape[0], -1)) # We use a log-softmax, to get log-probabilities\r\n",
        "            top_p, top_class = logprobs.topk(1, dim=1)\r\n",
        "            equals = (top_class == labels.view(images.shape[0], 1))\r\n",
        "            accuracy += torch.mean(equals.type(torch.FloatTensor))     \r\n",
        "        print(\"Accuracy %f\" %(accuracy/len(loader)))\r\n",
        "        return accuracy/len(loader)  \r\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nloclzQLUwrB"
      },
      "source": [
        "### Create two different models for testing\n",
        "\n",
        "* `net_batchnorm` uses batch normalization applied to the output of its hidden layers\n",
        "* `net_no_norm` does not use batch normalization\n",
        "\n",
        "Besides the normalization layers, everthing about these models is the same."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AFQ2pcH8UwrG"
      },
      "source": [
        "> **Exercise:** Train both models and compare the evolution of the train/validation loss in both cases"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xfjYlvmHasYs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1f37cb0-2906-48fe-ffd3-cd2aa3512b95"
      },
      "source": [
        "\r\n",
        "\r\n",
        "net_batchnorm = MLP_extended(dimx=784, hidden1=128,hidden2=64, nlabels=10,use_batch_norm=True,epochs=10,lr=1e-3)\r\n",
        "\r\n",
        "net_batchnorm.trainloop(trainloader, validloader)\r\n",
        "\r\n",
        "print(\"Train Accuracy %f\" %(net_batchnorm.accuracy(trainloader)))\r\n",
        "print(\"Valid Accuracy %f\" %(net_batchnorm.accuracy(validloader)))\r\n",
        "print(\"Test Accuracy %f\" %(net_batchnorm.accuracy(testloader)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training loss after 0 iterations: 0.290957\n",
            "Validation loss after 0 epochs: 0.113285\n",
            "Training loss after 1 iterations: 0.104279\n",
            "Validation loss after 1 epochs: 0.101893\n",
            "Training loss after 2 iterations: 0.073004\n",
            "Validation loss after 2 epochs: 0.089217\n",
            "Training loss after 3 iterations: 0.055506\n",
            "Validation loss after 3 epochs: 0.085939\n",
            "Training loss after 4 iterations: 0.046058\n",
            "Validation loss after 4 epochs: 0.082477\n",
            "Training loss after 5 iterations: 0.036773\n",
            "Validation loss after 5 epochs: 0.077693\n",
            "Training loss after 6 iterations: 0.031783\n",
            "Validation loss after 6 epochs: 0.075806\n",
            "Training loss after 7 iterations: 0.029539\n",
            "Validation loss after 7 epochs: 0.084594\n",
            "Training loss after 8 iterations: 0.026864\n",
            "Validation loss after 8 epochs: 0.080450\n",
            "Training loss after 9 iterations: 0.023114\n",
            "Validation loss after 9 epochs: 0.073443\n",
            "Accuracy 0.995750\n",
            "Train Accuracy 0.995750\n",
            "Accuracy 0.975399\n",
            "Valid Accuracy 0.975399\n",
            "Accuracy 0.976214\n",
            "Test Accuracy 0.976214\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wxYtgec4J5IG",
        "outputId": "17ff2fdc-2564-4da3-8a09-84c750361561"
      },
      "source": [
        "\r\n",
        "\r\n",
        "net_no_norm = MLP_extended(dimx=784, hidden1=128,hidden2=64, nlabels=10, use_batch_norm = False ,epochs=10,lr=1e-3)\r\n",
        "\r\n",
        "net_no_norm.trainloop(trainloader, validloader)\r\n",
        "\r\n",
        "print(\"Train Accuracy %f\" %(net_no_norm.accuracy(trainloader)))\r\n",
        "print(\"Valid Accuracy %f\" %(net_no_norm.accuracy(validloader)))\r\n",
        "print(\"Test Accuracy %f\" %(net_no_norm.accuracy(testloader)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training loss after 0 iterations: 0.463631\n",
            "Validation loss after 0 epochs: 0.306246\n",
            "Training loss after 1 iterations: 0.235319\n",
            "Validation loss after 1 epochs: 0.235725\n",
            "Training loss after 2 iterations: 0.173280\n",
            "Validation loss after 2 epochs: 0.203273\n",
            "Training loss after 3 iterations: 0.139725\n",
            "Validation loss after 3 epochs: 0.177059\n",
            "Training loss after 4 iterations: 0.116717\n",
            "Validation loss after 4 epochs: 0.139388\n",
            "Training loss after 5 iterations: 0.098232\n",
            "Validation loss after 5 epochs: 0.156430\n",
            "Training loss after 6 iterations: 0.086279\n",
            "Validation loss after 6 epochs: 0.133406\n",
            "Training loss after 7 iterations: 0.076281\n",
            "Validation loss after 7 epochs: 0.127400\n",
            "Training loss after 8 iterations: 0.071329\n",
            "Validation loss after 8 epochs: 0.138808\n",
            "Training loss after 9 iterations: 0.061316\n",
            "Validation loss after 9 epochs: 0.149094\n",
            "Accuracy 0.977578\n",
            "Train Accuracy 0.977578\n",
            "Accuracy 0.957083\n",
            "Valid Accuracy 0.957083\n",
            "Accuracy 0.961286\n",
            "Test Accuracy 0.961286\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fgDu0rSKbt2C"
      },
      "source": [
        "As expected, it can be seen that in the case with Batch Normalization, the accuracy is higher in both train, validation and test sets. Moreover, the loss functions are way lower than in the case without BN."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UfyqTIjVUwrR"
      },
      "source": [
        "---\n",
        "### Considerations for other network types\n",
        "\n",
        "This notebook demonstrates batch normalization in a standard neural network with fully connected layers. You can also use batch normalization in other types of networks, but there are some special considerations.\n",
        "\n",
        "#### ConvNets\n",
        "\n",
        "Convolution layers consist of multiple feature maps. (Remember, the depth of a convolutional layer refers to its number of feature maps.) And the weights for each feature map are shared across all the inputs that feed into the layer. Because of these differences, batch normalizing convolutional layers requires batch/population mean and variance per feature map rather than per node in the layer.\n",
        "\n",
        "> To apply batch normalization on the outputs of convolutional layers, we use [BatchNorm2d](https://pytorch.org/docs/stable/nn.html#batchnorm2d). To use it, we simply state the **number of input feature maps**. I.e. `nn.BatchNorm2d(num_features=nmaps)`\n",
        "\n",
        "\n",
        "#### RNNs\n",
        "\n",
        "Batch normalization can work with recurrent neural networks, too, as shown in the 2016 paper [Recurrent Batch Normalization](https://arxiv.org/abs/1603.09025). It's a bit more work to implement, but basically involves calculating the means and variances per time step instead of per layer. You can find an example where someone implemented recurrent batch normalization in PyTorch, in [this GitHub repo](https://github.com/jihunchoi/recurrent-batch-normalization-pytorch)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXRabJ90UwrT"
      },
      "source": [
        "> **Exercise:** Using CIFAR10 database, incorporate BN to your solution of Lab 3 (Part I). Compare the results with and without BN!!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 116,
          "referenced_widgets": [
            "360711691d4e4aaf942b0a8273e5cab9",
            "06d05010c743477fadeb4329a78b2d0b",
            "359e98ccd0a44c63958b07f060aafb7c",
            "19cb2cb72cbb48b7bc0a2dc3acd834a9",
            "04284fe814424b0e9b40f6599a649d20",
            "c415d16c8275416cb1b7fdf8dc5d2052",
            "d9e8bd121ad0477c91d87dd8a045907e",
            "41060d57b0fd44eab02949f1bd8a4304"
          ]
        },
        "id": "Nq53KC-tTxQq",
        "outputId": "a5c053fa-2223-4760-b77c-44ea24231695"
      },
      "source": [
        "\r\n",
        "import torch\r\n",
        "from torchvision import datasets, transforms, utils\r\n",
        "\r\n",
        "transform = transforms.Compose(\r\n",
        "    [transforms.ToTensor(),\r\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\r\n",
        "\r\n",
        "trainset2 = datasets.CIFAR10(root='./data', train=True,\r\n",
        "                                        download=True, transform=transform)\r\n",
        "\r\n",
        "trainloader2 = torch.utils.data.DataLoader(trainset2, batch_size=64,\r\n",
        "                                          shuffle=True, num_workers=2)\r\n",
        "\r\n",
        "testset2 = datasets.CIFAR10(root='./data', train=False,\r\n",
        "                                       download=True, transform=transform)\r\n",
        "\r\n",
        "testloader2 = torch.utils.data.DataLoader(testset2, batch_size=64,\r\n",
        "                                         shuffle=False, num_workers=2)\r\n",
        "\r\n",
        "classes = ('plane', 'car', 'bird', 'cat',\r\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "360711691d4e4aaf942b0a8273e5cab9",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=170498071.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGw6GzXNT4DD"
      },
      "source": [
        "\r\n",
        "\r\n",
        "import copy\r\n",
        "\r\n",
        "numval2 = int(len(trainset2)*0.8)\r\n",
        "\r\n",
        "validloader2 = copy.deepcopy(trainloader2)  # Creates a copy of the object \r\n",
        "full_trainloader2 = copy.deepcopy(trainloader2)\r\n",
        "\r\n",
        "\r\n",
        "#We take the first  images for training\r\n",
        "trainloader2.dataset.data = trainloader2.dataset.data[:numval2,:,:]\r\n",
        "trainloader2.dataset.targets = trainloader2.dataset.targets[:numval2]\r\n",
        "\r\n",
        "#And the rest for validation\r\n",
        "validloader2.dataset.data = validloader2.dataset.data[numval2:,:,:]\r\n",
        "validloader2.dataset.targets = validloader2.dataset.targets[numval2:]\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LLHhFPGpXQLk"
      },
      "source": [
        "\r\n",
        "\r\n",
        "class Lenet5(nn.Module):\r\n",
        "    def __init__(self,dimx,use_batch_norm,nlabels, pr=0.2): #Nlabels will be 10 in our case\r\n",
        "        super().__init__()\r\n",
        "        # Keep track of whether or not this network uses batch normalization.\r\n",
        "        self.use_batch_norm = use_batch_norm\r\n",
        "        # convolutional layer (sees 28x28x1 image tensor)\r\n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=6, \r\n",
        "                               kernel_size=5, stride=1, padding=0)     \r\n",
        "        # convolutional layer (sees 12x12x16 tensor)\r\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5, padding=0)      \r\n",
        "        # Max pool layer\r\n",
        "        self.pool = nn.MaxPool2d(2, 2)\r\n",
        "        # Linear layers\r\n",
        "        self.linear1 = nn.Linear(in_features = 400, out_features = 120) #      \r\n",
        "        self.linear2 = nn.Linear(in_features = 120, out_features = 84) #       \r\n",
        "        self.linear3 = nn.Linear(in_features = 84, out_features = 10) #\r\n",
        "        self.relu = nn.ReLU()      \r\n",
        "        self.logsoftmax = nn.LogSoftmax(dim=1)     \r\n",
        "        # adding the dropout component:\r\n",
        "        self.dropout = nn.Dropout(pr)\r\n",
        "\r\n",
        "        if self.use_batch_norm:\r\n",
        "            self.batch_norm1 = nn.BatchNorm2d(num_features=6)\r\n",
        "            self.batch_norm2 = nn.BatchNorm2d(num_features=16)  \r\n",
        "            self.batch_norm3 = nn.BatchNorm1d(120) # first linear\r\n",
        "            self.batch_norm4 = nn.BatchNorm1d(84) # second linear         \r\n",
        "\r\n",
        "        # Spatial dimension of the Tensor at the output of the 2nd CNN\r\n",
        "        self.final_dim = int(((dimx-4)/2-4)/2)\r\n",
        "        \r\n",
        "    def forward(self, x):\r\n",
        "        # Pass the input tensor through the CNN operations\r\n",
        "        x = self.conv1(x)\r\n",
        "        if self.use_batch_norm:\r\n",
        "            x = self.batch_norm1(x) \r\n",
        "        x = self.relu(x) \r\n",
        "        x = self.pool(x)\r\n",
        "        x = self.conv2(x)\r\n",
        "        if self.use_batch_norm:\r\n",
        "            x = self.batch_norm2(x)\r\n",
        "        x = self.relu(x)\r\n",
        "        x = self.pool(x)\r\n",
        "        # Flatten the tensor into a vector of appropiate dimension using self.final_dim\r\n",
        "       # x = x.view(-1, 16*self.final_dim*self.final_dim)\r\n",
        "        x = x.view(x.size(0), -1)\r\n",
        "        # Pass the tensor through the Dense Layers\r\n",
        "        x = self.linear1(x) #\r\n",
        "        if self.use_batch_norm:   # batch normalization #3\r\n",
        "          x = self.batch_norm3(x)\r\n",
        "        x = self.relu(x)\r\n",
        "        x = self.dropout(x) \r\n",
        "        x = self.linear2(x) #    \r\n",
        "        if self.use_batch_norm:   # batch normalization #4\r\n",
        "          x = self.batch_norm4(x)  \r\n",
        "        x = self.relu(x)\r\n",
        "        x = self.dropout(x) \r\n",
        "        x = self.linear3(x) #\r\n",
        "        x = self.logsoftmax(x) \r\n",
        "        return x\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_fCIjeBu08e7"
      },
      "source": [
        "class Lenet5_extended(Lenet5):    \r\n",
        "    def __init__(self,dimx,nlabels,use_batch_norm,epochs=100,lr=0.001): \r\n",
        "        super().__init__(dimx,nlabels,use_batch_norm, pr = 0.3)  #To initialize Lenet5!  \r\n",
        "        self.lr = lr #Learning Rate\r\n",
        "        self.optim = optim.Adam(self.parameters(), self.lr) \r\n",
        "        self.epochs = epochs   \r\n",
        "        self.criterion = nn.NLLLoss()\r\n",
        "       # A list to store the loss evolution along training and validation\r\n",
        "        self.loss_during_training = [] \r\n",
        "        self.valid_loss_during_training = []\r\n",
        "\r\n",
        "\r\n",
        "    def valid_loss(self, validloader): \r\n",
        "        if validloader is not None:  \r\n",
        "            with torch.no_grad():\r\n",
        "                self.eval()\r\n",
        "                running_loss = 0.    \r\n",
        "                for (images, labels) in validloader:\r\n",
        "                   # out = self.forward(images.view(images.shape[0], -1))\r\n",
        "                    out = self.forward(images)\r\n",
        "                    loss = self.criterion(out, labels)\r\n",
        "                    running_loss += loss.item()    \r\n",
        "                self.valid_loss_during_training.append(running_loss/len(validloader))    \r\n",
        "        else:  \r\n",
        "            raise ValueError('validloader must contain data.')    \r\n",
        "\r\n",
        "    # set model back to train mode\r\n",
        "        self.train() \r\n",
        "\r\n",
        "    def trainloop(self,trainloader, validloader = None):       \r\n",
        "        # Adam Loop       \r\n",
        "        for e in range(int(self.epochs)):\r\n",
        "            running_loss = 0.\r\n",
        "            for (images, labels) in trainloader:             \r\n",
        "\r\n",
        "                self.optim.zero_grad()  #TO RESET GRADIENTS!\r\n",
        "                # out = self.forward(images.view(images.shape[0], -1))\r\n",
        "                out = self.forward(images)\r\n",
        "                loss = self.criterion(out, labels)\r\n",
        "                running_loss += loss.item()\r\n",
        "                loss.backward()\r\n",
        "                self.optim.step()        \r\n",
        "            self.loss_during_training.append(running_loss/len(trainloader))\r\n",
        "            if validloader is not None:\r\n",
        "                self.valid_loss(validloader) \r\n",
        "\r\n",
        "            if(e % 1 == 0): # Every 1 epochs              \r\n",
        "                print(\"Training loss after %d iterations: %f\" \r\n",
        "                      %(e,self.loss_during_training[-1]))              \r\n",
        "                print(\"Validation loss after %d epochs: %f\" \r\n",
        "                      %(e,self.valid_loss_during_training[-1]))   \r\n",
        "\r\n",
        "    def accuracy(self, test_data):\r\n",
        "          accuracy = 0\r\n",
        "        # Turn off gradients for validation, saves memory and computations\r\n",
        "          with torch.no_grad():\r\n",
        "            for (images, labels) in test_data:\r\n",
        "                #logprobs = self.forward(images.view(images.shape[0], -1)) # We use a log-softmax, to get log-probabilities\r\n",
        "                logprobs = self.forward(images)\r\n",
        "                top_p, top_class = logprobs.topk(1, dim=1)\r\n",
        "                equals = (top_class == labels.view(images.shape[0], 1))\r\n",
        "                accuracy += torch.mean(equals.type(torch.FloatTensor))\r\n",
        "                \r\n",
        "          print(\"Accuracy %f\" %(accuracy/len(test_data)))\r\n",
        "          return accuracy/len(test_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "56aCTYOl3yLf",
        "outputId": "bd2ee383-2978-4376-bf1f-30169a9633ca"
      },
      "source": [
        "my_CNN_Lenet = Lenet5_extended(dimx = 32, nlabels = 10, use_batch_norm = True, epochs = 20, lr = 1e-3)\r\n",
        "my_CNN_Lenet.trainloop(trainloader2, validloader2)\r\n",
        "\r\n",
        "print(\"Train data:\")\r\n",
        "my_CNN_Lenet.accuracy(full_trainloader2)\r\n",
        "print(\"Test data:\")\r\n",
        "my_CNN_Lenet.accuracy(testloader2)  #"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training loss after 0 iterations: 1.631382\n",
            "Validation loss after 0 epochs: 1.413077\n",
            "Training loss after 1 iterations: 1.359642\n",
            "Validation loss after 1 epochs: 1.219037\n",
            "Training loss after 2 iterations: 1.251185\n",
            "Validation loss after 2 epochs: 1.147089\n",
            "Training loss after 3 iterations: 1.186236\n",
            "Validation loss after 3 epochs: 1.104796\n",
            "Training loss after 4 iterations: 1.136592\n",
            "Validation loss after 4 epochs: 1.061871\n",
            "Training loss after 5 iterations: 1.097977\n",
            "Validation loss after 5 epochs: 1.029301\n",
            "Training loss after 6 iterations: 1.071666\n",
            "Validation loss after 6 epochs: 1.021443\n",
            "Training loss after 7 iterations: 1.039811\n",
            "Validation loss after 7 epochs: 1.022729\n",
            "Training loss after 8 iterations: 1.022765\n",
            "Validation loss after 8 epochs: 1.060643\n",
            "Training loss after 9 iterations: 1.004440\n",
            "Validation loss after 9 epochs: 0.999715\n",
            "Training loss after 10 iterations: 0.982071\n",
            "Validation loss after 10 epochs: 0.990055\n",
            "Training loss after 11 iterations: 0.967940\n",
            "Validation loss after 11 epochs: 1.001126\n",
            "Training loss after 12 iterations: 0.948882\n",
            "Validation loss after 12 epochs: 0.971675\n",
            "Training loss after 13 iterations: 0.928597\n",
            "Validation loss after 13 epochs: 0.958506\n",
            "Training loss after 14 iterations: 0.921827\n",
            "Validation loss after 14 epochs: 0.958936\n",
            "Training loss after 15 iterations: 0.900467\n",
            "Validation loss after 15 epochs: 0.973087\n",
            "Training loss after 16 iterations: 0.899467\n",
            "Validation loss after 16 epochs: 1.014515\n",
            "Training loss after 17 iterations: 0.884651\n",
            "Validation loss after 17 epochs: 0.964826\n",
            "Training loss after 18 iterations: 0.875062\n",
            "Validation loss after 18 epochs: 0.958899\n",
            "Training loss after 19 iterations: 0.864985\n",
            "Validation loss after 19 epochs: 0.970505\n",
            "Train data:\n",
            "Accuracy 0.692575\n",
            "Test data:\n",
            "Accuracy 0.611166\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.6112)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tAmnmf9uqoGJ",
        "outputId": "7a89a225-5a71-482b-a814-3b74f4f42107"
      },
      "source": [
        "my_CNN_Lenet = Lenet5_extended(dimx = 32, nlabels = 10, use_batch_norm = False, epochs = 20, lr = 1e-3)\r\n",
        "my_CNN_Lenet.trainloop(trainloader2, validloader2)\r\n",
        "\r\n",
        "print(\"Train data:\")\r\n",
        "my_CNN_Lenet.accuracy(full_trainloader2)\r\n",
        "print(\"Test data:\")\r\n",
        "my_CNN_Lenet.accuracy(testloader2)  #"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training loss after 0 iterations: 1.604086\n",
            "Validation loss after 0 epochs: 1.316669\n",
            "Training loss after 1 iterations: 1.328411\n",
            "Validation loss after 1 epochs: 1.287307\n",
            "Training loss after 2 iterations: 1.221542\n",
            "Validation loss after 2 epochs: 1.125470\n",
            "Training loss after 3 iterations: 1.161390\n",
            "Validation loss after 3 epochs: 1.063552\n",
            "Training loss after 4 iterations: 1.105915\n",
            "Validation loss after 4 epochs: 1.019644\n",
            "Training loss after 5 iterations: 1.072462\n",
            "Validation loss after 5 epochs: 1.007428\n",
            "Training loss after 6 iterations: 1.036811\n",
            "Validation loss after 6 epochs: 1.017430\n",
            "Training loss after 7 iterations: 1.007060\n",
            "Validation loss after 7 epochs: 1.005187\n",
            "Training loss after 8 iterations: 0.989100\n",
            "Validation loss after 8 epochs: 1.011226\n",
            "Training loss after 9 iterations: 0.967621\n",
            "Validation loss after 9 epochs: 0.964540\n",
            "Training loss after 10 iterations: 0.951535\n",
            "Validation loss after 10 epochs: 1.016596\n",
            "Training loss after 11 iterations: 0.928738\n",
            "Validation loss after 11 epochs: 0.935766\n",
            "Training loss after 12 iterations: 0.917657\n",
            "Validation loss after 12 epochs: 0.951804\n",
            "Training loss after 13 iterations: 0.902795\n",
            "Validation loss after 13 epochs: 0.970531\n",
            "Training loss after 14 iterations: 0.889439\n",
            "Validation loss after 14 epochs: 0.949265\n",
            "Training loss after 15 iterations: 0.876918\n",
            "Validation loss after 15 epochs: 0.969456\n",
            "Training loss after 16 iterations: 0.866023\n",
            "Validation loss after 16 epochs: 0.996017\n",
            "Training loss after 17 iterations: 0.860132\n",
            "Validation loss after 17 epochs: 0.927818\n",
            "Training loss after 18 iterations: 0.851234\n",
            "Validation loss after 18 epochs: 0.952362\n",
            "Training loss after 19 iterations: 0.845423\n",
            "Validation loss after 19 epochs: 0.935589\n",
            "Train data:\n",
            "Accuracy 0.699089\n",
            "Test data:\n",
            "Accuracy 0.621517\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.6215)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2Zyj1OEuBIF"
      },
      "source": [
        "From the analysis, it is seen that the accuracies in the train and test data with and without Batch Normalization is are similar, nevertheless, the model without BN is slightly more accurate. This issue happens sometimes, it is not quite strange, and can be caused by several facts: the objective function is not correctly smoothed, or maybe the intermmediate layers don't work as expected. "
      ]
    }
  ]
}
