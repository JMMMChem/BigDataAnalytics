{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN with random initialization Fake News.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cVkr5MDnxZ4Z"
      },
      "source": [
        "## **CNN Analysis for fake news detection with random initialization**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aChHDOHlxj20"
      },
      "source": [
        " **Load the required libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3NmGZCTvxYAo",
        "outputId": "3b753020-85a4-43a6-e7ac-825b78599248"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import nltk\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "stopwords_en = stopwords.words(\"english\")\n",
        "\n",
        "import re\n",
        "\n",
        "# Importing required libraries\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# keras\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "from keras.layers import Embedding\n",
        "\n",
        "from keras.layers import Dense, Input, GlobalMaxPooling1D\n",
        "from keras.layers import Conv1D, MaxPooling1D\n",
        "from keras.layers import Embedding, Conv2D, MaxPooling2D, Dropout,concatenate\n",
        "from keras.layers import Activation, Flatten, Bidirectional, GlobalMaxPool1D\n",
        "from keras.models import Model, Sequential\n",
        "import keras\n",
        "from keras.layers.core import Reshape, Flatten\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.optimizers import SGD, RMSprop, Adagrad, Adadelta, Adam, Adamax, Nadam\n",
        "from keras.models import Model\n",
        "from keras import regularizers\n",
        "\n",
        "# gensim\n",
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.utils import simple_preprocess\n",
        "\n",
        "from gensim.models.keyedvectors import KeyedVectors"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXvAKa0PxuIy"
      },
      "source": [
        "**Data loading and splitting**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vHyaviJBxpAJ",
        "outputId": "7ea22ac8-7588-40a6-a5eb-3627044c4e90"
      },
      "source": [
        "# Load the Drive helper and mount\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EtiqSHQ3xrmN"
      },
      "source": [
        "    # Load the train dataset\n",
        "cod_train = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/fake-news/train.csv\")\n",
        "    # split for cross-validation (train-64%, validation 16% and test 20%)\n",
        "cod_train2, test= train_test_split(cod_train, random_state=0, test_size=0.2)\n",
        "train, val =  train_test_split(cod_train2, random_state = 0,test_size=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_-3aFRkxvGh"
      },
      "source": [
        "**Tokenization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y6AJKUBfxyVh",
        "outputId": "65ecc9f2-c1ec-491f-e778-0897bfcc7d34"
      },
      "source": [
        "max_features = 100000 # max num words\n",
        "maxlen = 250 \n",
        "embedding_size = 200\n",
        "\n",
        "# create the tokenizer with the maximum number of words to keep, \n",
        "# based on word frequency. \n",
        "# Only the most common num_words-1 words will be kept.\n",
        "tokenizer = Tokenizer(num_words=max_features, oov_token = True)\n",
        "\n",
        "train['text']=train['text'].astype(str)\n",
        "test['text']=test['text'].astype(str)\n",
        "val['text']=val['text'].astype(str)\n",
        "\n",
        "# fit the tokenizer on the texts\n",
        "tokenizer.fit_on_texts(list(train['text']))\n",
        "\n",
        "# Transforms each text in texts to a sequence of integers.\n",
        "train_X = tokenizer.texts_to_sequences(train['text'])\n",
        "test_X = tokenizer.texts_to_sequences(test['text'])\n",
        "val_X = tokenizer.texts_to_sequences(val['text'])\n",
        "\n",
        "# transforms a list of num_samples sequences (lists of integers)\n",
        "# into a 2D Numpy array of shape (num_samples, num_timesteps).\n",
        "train_X = pad_sequences(train_X, maxlen = maxlen)\n",
        "test_X = pad_sequences(test_X, maxlen = maxlen)\n",
        "val_X = pad_sequences(val_X, maxlen = maxlen)\n",
        "\n",
        "train_y = train['label']\n",
        "test_y = test['label']\n",
        "val_y = val['label']\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  # Remove the CWD from sys.path while we load stuff.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  # This is added back by InteractiveShellApp.init_path()\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  if sys.path[0] == '':\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7NIrXqATyxoK"
      },
      "source": [
        "**Constructing the CNN with random initialization**\n",
        "\n",
        "It is composed of 2 convolutional networks of 1 dimension, a max pooling, and a global max pooling layer.\n",
        "\n",
        "- Activation Function: ReLU has been used as the activation function. It is a non-linear activation function, helping complex relationships in the data to be captured by the model.\n",
        "\n",
        "- Optimizer: Adam optimizer, an adaptive learning rate optimizer.\n",
        "\n",
        "- Loss function: The network will be trained to output a probability over the 2 classes using Sigmoid Loss."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_nffyl-t6s_"
      },
      "source": [
        "#CONV1D \n",
        "num_filters = 200\n",
        "sequence_length = train_X.shape[1]\n",
        "weight_decay = 1e-4\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_features, embedding_size, input_length = sequence_length))\n",
        "model.add(Conv1D(num_filters, 7, activation='relu', padding='same'))\n",
        "model.add(MaxPooling1D(2))\n",
        "model.add(Conv1D(num_filters, 7, activation='relu', padding='same'))\n",
        "model.add(GlobalMaxPooling1D())\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(32, activation='relu', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model.add(Dense(1, activation='sigmoid'))  #multi-label (k-hot encoding)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P0GmfFSH7Vmb"
      },
      "source": [
        "val_y22 = np.array(val_y, dtype=np.float)\n",
        "val_x22 = np.array(val_X, dtype=np.float)\n",
        "test_y22 = np.array(test_y, dtype=np.float)\n",
        "test_x22 = np.array(test_X, dtype=np.float)\n",
        "train_y22 = np.array(train_y, dtype=np.float)\n",
        "train_x22 = np.array(train_X, dtype=np.float)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Agd2BgMay3rA"
      },
      "source": [
        "and compiling it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pn-UxTYkx4t7",
        "outputId": "490c94f3-fa6c-452b-993c-ab16afc20eb6"
      },
      "source": [
        "#FOR CONV1D \n",
        "\n",
        "# Compiling Model using optimizer\n",
        "opt = Adam(lr=1e-3)\n",
        "model.compile(loss='binary_crossentropy',optimizer=opt, metrics=['accuracy',keras.metrics.Precision(), keras.metrics.Recall(), keras.metrics.TruePositives()])\n",
        "#model.summary()\n",
        "\n",
        "# Fitting Model to the data\n",
        "# Save the model after every epoch.\n",
        "saveBestModel = keras.callbacks.ModelCheckpoint('/content/drive/My Drive/TFMColab/best_model.hdf5', monitor='val_acc', verbose=0, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
        "# Stop training when a monitored quantity has stopped improving.\n",
        "earlyStopping = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\n",
        "batch_size = 200\n",
        "epochs = 5\n",
        "\n",
        "model.fit(train_X, train_y, batch_size=batch_size, epochs=epochs, validation_data=(val_X, val_y), callbacks=[saveBestModel, earlyStopping])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "Epoch 1/5\n",
            "67/67 [==============================] - 52s 271ms/step - loss: 0.5772 - accuracy: 0.6683 - precision: 0.7024 - recall: 0.5995 - true_positives: 2119.1765 - val_loss: 0.1920 - val_accuracy: 0.9237 - val_precision: 0.9380 - val_recall: 0.9076 - val_true_positives: 1512.0000\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 2/5\n",
            "67/67 [==============================] - 17s 254ms/step - loss: 0.1182 - accuracy: 0.9595 - precision: 0.9621 - recall: 0.9549 - true_positives: 3257.6912 - val_loss: 0.1562 - val_accuracy: 0.9417 - val_precision: 0.9688 - val_recall: 0.9130 - val_true_positives: 1521.0000\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 3/5\n",
            "67/67 [==============================] - 17s 258ms/step - loss: 0.0299 - accuracy: 0.9935 - precision: 0.9943 - recall: 0.9928 - true_positives: 3421.3824 - val_loss: 0.2072 - val_accuracy: 0.9357 - val_precision: 0.9226 - val_recall: 0.9514 - val_true_positives: 1585.0000\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 4/5\n",
            "67/67 [==============================] - 17s 254ms/step - loss: 0.0107 - accuracy: 0.9981 - precision: 0.9979 - recall: 0.9982 - true_positives: 3419.3971 - val_loss: 0.2338 - val_accuracy: 0.9363 - val_precision: 0.9374 - val_recall: 0.9352 - val_true_positives: 1558.0000\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 5/5\n",
            "67/67 [==============================] - 17s 254ms/step - loss: 0.0085 - accuracy: 0.9988 - precision: 0.9988 - recall: 0.9989 - true_positives: 3453.9265 - val_loss: 0.2495 - val_accuracy: 0.9402 - val_precision: 0.9384 - val_recall: 0.9424 - val_true_positives: 1570.0000\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f7d6c8eb0d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I0yn7yTIyja_"
      },
      "source": [
        "Then the **metrics** are computed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oRphzfCOx63J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b8a1007-afb8-45e6-b7b6-7ed92950cd9d"
      },
      "source": [
        "loss, accuracy, precision, recall, true_positives = model.evaluate(test_X, test_y, batch_size=batch_size)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "21/21 [==============================] - 1s 28ms/step - loss: 0.2532 - accuracy: 0.9406 - precision: 0.9456 - recall: 0.9371 - true_positives: 1981.0000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t7ofDlrNx8YT"
      },
      "source": [
        "mult_pr=precision*recall\n",
        "sum_pr=precision+recall\n",
        "div=mult_pr/sum_pr\n",
        "f1_score=2*div"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vOOxcP0qx-Du",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5fa6da3-71ec-4bfb-afd0-49e78ab057ba"
      },
      "source": [
        "print('Loss:',loss)\n",
        "print('Accuracy:',accuracy)\n",
        "print('Precision:',precision)\n",
        "print('Recall:',recall)\n",
        "print('f1 score:',f1_score)\n",
        "print('True positives:',true_positives)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss: 0.2532398998737335\n",
            "Accuracy: 0.940625011920929\n",
            "Precision: 0.9455847144126892\n",
            "Recall: 0.9370861053466797\n",
            "f1 score: 0.9413162279931595\n",
            "True positives: 1981.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-gtYThJdx_mZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17b2893a-b653-4ae4-b14b-ce05fa0bc598"
      },
      "source": [
        "pred_y = model.predict(test_X, batch_size=batch_size)\n",
        "pred_y\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[7.2118390e-05],\n",
              "       [9.9998295e-01],\n",
              "       [1.0000000e+00],\n",
              "       ...,\n",
              "       [3.3901333e-06],\n",
              "       [9.9206626e-01],\n",
              "       [9.9999976e-01]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I-ZA0bkQyBJj"
      },
      "source": [
        "pred_y2 = np.round(pred_y, decimals = 1)\n",
        "pred_y2=pred_y2.astype('int64')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NLhUOlxgymfM"
      },
      "source": [
        "And the **confusion** **matrix**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jDeHKVILyCfx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "940c070c-7113-4836-846a-462ef7805064"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "confusion_matrix(test_y, pred_y2)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1989,   57],\n",
              "       [ 278, 1836]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3o8OeCAyusi"
      },
      "source": [
        "Finally, **false** **positives** and **false** **negatives** are stored in two csv files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dy_L8f1vyFtT"
      },
      "source": [
        "def getFP_FN_lists(test_X, test_y, pred_y):\n",
        "    FP_text = []\n",
        "    FP_index = []\n",
        "    FN_text = []\n",
        "    FN_index = []\n",
        "    for i in range(len(test_y)):\n",
        "        if(pred_y2[i]==1 and test_y[test_y.index[i]]==0):\n",
        "            FP_text.append(test['text'][test_y.index[i]])\n",
        "            FP_index.append(test_y.index[i])\n",
        "        elif(pred_y2[i]==0 and test_y[test_y.index[i]]==1):\n",
        "            FN_text.append(test['text'][test_y.index[i]])\n",
        "            FN_index.append(test_y.index[i])\n",
        "            \n",
        "    return FP_text,FP_index,FN_text,FN_index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tyz9wJiayHAd"
      },
      "source": [
        "def getFP_FN(test_X, test_y, pred_y2):\n",
        "    FP_text,FP_index,FN_text,FN_index = getFP_FN_lists(test_X, test_y, pred_y2)\n",
        "    d_FP = {'FP_text':FP_text,'FP_index':FP_index}\n",
        "    df_FP = pd.DataFrame(d_FP)\n",
        "    d_FN = {'FN_text':FN_text,'FN_index':FN_index}\n",
        "    df_FN = pd.DataFrame(d_FN)\n",
        "   \n",
        "    return df_FP,df_FN"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HlfF1kl-yHqT"
      },
      "source": [
        "df_FP,df_FN = getFP_FN(test_X, test_y, pred_y2)\n",
        "df_FP.to_csv('FP_CNN2.csv', index=True)\n",
        "df_FN.to_csv('FN_CNN2.csv', index=True)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
