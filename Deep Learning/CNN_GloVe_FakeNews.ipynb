{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN GloVe FakeNews.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hh2uqJ8czP9D"
      },
      "source": [
        "## **CNN Analysis for fake news detection**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4TM23OuEzW8w"
      },
      "source": [
        " **Load the required libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MClfuEJryxNu",
        "outputId": "029299c5-f7fe-4dab-a57f-b841deca0ce5"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import nltk\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "stopwords_en = stopwords.words(\"english\")\n",
        "\n",
        "import re\n",
        "\n",
        "# Importing required libraries\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# keras\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "from keras.layers import Embedding\n",
        "\n",
        "from keras.layers import Dense, Input, GlobalMaxPooling1D\n",
        "from keras.layers import Conv1D, MaxPooling1D\n",
        "from keras.layers import Embedding, Conv2D, MaxPooling2D, Dropout,concatenate\n",
        "from keras.layers import Activation, Flatten, Bidirectional, GlobalMaxPool1D\n",
        "from keras.models import Model, Sequential\n",
        "import keras\n",
        "from keras.layers.core import Reshape, Flatten\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.optimizers import SGD, RMSprop, Adagrad, Adadelta, Adam, Adamax, Nadam\n",
        "from keras.models import Model\n",
        "from keras import regularizers\n",
        "\n",
        "# gensim\n",
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.utils import simple_preprocess\n",
        "\n",
        "from gensim.models.keyedvectors import KeyedVectors"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_SJ6n2uzkRD"
      },
      "source": [
        "**Data loading and splitting**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LTkH9FJP2cI0",
        "outputId": "dd7b67be-ed26-4fda-e4c3-26b90625b348"
      },
      "source": [
        "# Load the Drive helper and mount\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PHoZG7kO5u3T"
      },
      "source": [
        "    # Load the train dataset\n",
        "cod_train = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/fake-news/train\")\n",
        "    # split for cross-validation (train-64%, validation 16% and test 20%)\n",
        "cod_train2, test= train_test_split(cod_train, random_state=0, test_size=0.2)\n",
        "train, val =  train_test_split(cod_train2, random_state = 0,test_size=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZFgs4tsg6W8q"
      },
      "source": [
        "**Tokenization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49JPkSUE2Zpv",
        "outputId": "101eebee-7659-4934-8a91-8e11356efbc5"
      },
      "source": [
        "max_features = 100000 # max num words\n",
        "maxlen = 250 \n",
        "embedding_size = 200\n",
        "\n",
        "# create the tokenizer with the maximum number of words to keep, \n",
        "# based on word frequency. \n",
        "# Only the most common num_words-1 words will be kept.\n",
        "tokenizer = Tokenizer(num_words=max_features, oov_token = True)\n",
        "\n",
        "train['text']=train['text'].astype(str)\n",
        "test['text']=test['text'].astype(str)\n",
        "val['text']=val['text'].astype(str)\n",
        "\n",
        "# fit the tokenizer on the texts\n",
        "tokenizer.fit_on_texts(list(train['text']))\n",
        "\n",
        "# Transforms each text in texts to a sequence of integers.\n",
        "train_X = tokenizer.texts_to_sequences(train['text'])\n",
        "test_X = tokenizer.texts_to_sequences(test['text'])\n",
        "val_X = tokenizer.texts_to_sequences(val['text'])\n",
        "\n",
        "# transforms a list of num_samples sequences (lists of integers)\n",
        "# into a 2D Numpy array of shape (num_samples, num_timesteps).\n",
        "train_X = pad_sequences(train_X, maxlen = maxlen)\n",
        "test_X = pad_sequences(test_X, maxlen = maxlen)\n",
        "val_X = pad_sequences(val_X, maxlen = maxlen)\n",
        "\n",
        "train_y = train['label']\n",
        "test_y = test['label']\n",
        "val_y = val['label']\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  # Remove the CWD from sys.path while we load stuff.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  # This is added back by InteractiveShellApp.init_path()\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  if sys.path[0] == '':\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQfOnn9MzouT"
      },
      "source": [
        "**Implementation of CNN using word embeddings using GloVe for text classification**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pv1mo6_bz5mS",
        "outputId": "7a0e0bbe-697c-49e7-9ff1-ad6ed920eec1"
      },
      "source": [
        "# load embeddings\n",
        "EMBEDDING_FILE = '/content/drive/MyDrive/Colab Notebooks/fake-news/glove.6B.200d.txt'\n",
        "\n",
        "def get_coefs(word,*arr): \n",
        "    return word, np.asarray(arr, dtype='float32')\n",
        "\n",
        "# first, build index mapping words in the embeddings set\n",
        "# to their embedding vector\n",
        "embeddings_index = {}\n",
        "with open(EMBEDDING_FILE, encoding=\"utf8\") as f:\n",
        "    for line in f:\n",
        "        word, coefs = get_coefs(*line.split(\" \"))\n",
        "        embeddings_index[word] = coefs\n",
        "            \n",
        "all_embs = np.stack(embeddings_index.values())\n",
        "emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
        "embed_size = all_embs.shape[1]\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "nb_words = min(max_features, len(word_index))\n",
        "\n",
        "# Random embedding vector for unknown words.\n",
        "embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embedding_size))\n",
        "# prepare embedding matrix\n",
        "for word, i in word_index.items():\n",
        "    if i >= max_features: \n",
        "        continue\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None: \n",
        "        # words not found in embedding index will be random\n",
        "        embedding_matrix[i] = embedding_vector"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2822: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
            "  if self.run_code(code, result):\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUkSEQUC7dAO"
      },
      "source": [
        "For the network itself:\n",
        "\n",
        "It is composed of 2 convolutional networks of 1 dimension, a max pooling, and a global max pooling layer.\n",
        "\n",
        "- Activation Function: ReLU has been used as the activation function. It is a non-linear activation function, helping complex relationships in the data to be captured by the model.\n",
        "\n",
        "- Optimizer: Adam optimizer, an adaptive learning rate optimizer.\n",
        "\n",
        "- Loss function: The network will be trained to output a probability over the 2 classes using Sigmoid Loss."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TGsSDOYozEer"
      },
      "source": [
        "#CONV1D\n",
        "num_filters = 200\n",
        "sequence_length = train_X.shape[1]\n",
        "weight_decay = 1e-4\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_features, embedding_size, weights = [embedding_matrix], input_length = sequence_length))\n",
        "model.add(Conv1D(num_filters, 7, activation='relu', padding='same'))\n",
        "model.add(MaxPooling1D(2))\n",
        "model.add(Conv1D(num_filters, 7, activation='relu', padding='same'))\n",
        "model.add(GlobalMaxPooling1D())\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(32, activation='relu', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model.add(Dense(1, activation='sigmoid'))  #multi-label (k-hot encoding)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lgofaOpeyNF3"
      },
      "source": [
        "Compiling the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jItkbJaiv-bX",
        "outputId": "0cd83350-c977-4a5e-8b15-3c9f6bb0e287"
      },
      "source": [
        "#FOR CONV1D \n",
        "\n",
        "# Compiling Model using optimizer\n",
        "opt = Adam(lr=1e-3)\n",
        "model.compile(loss='binary_crossentropy',optimizer=opt, metrics=['accuracy',keras.metrics.Precision(), keras.metrics.Recall(), keras.metrics.TruePositives()])\n",
        "\n",
        "# Fitting Model to the data\n",
        "# Save the model after every epoch.\n",
        "saveBestModel = keras.callbacks.ModelCheckpoint('/content/drive/My Drive/TFMColab/best_model.hdf5', monitor='val_acc', verbose=0, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
        "# Stop training when a monitored quantity has stopped improving.\n",
        "earlyStopping = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\n",
        "batch_size = 200\n",
        "epochs = 5\n",
        "model.fit(train_X, train_y, batch_size=batch_size, epochs=epochs, validation_data=(val_X, val_y), callbacks=[saveBestModel, earlyStopping])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "Epoch 1/5\n",
            "67/67 [==============================] - 207s 3s/step - loss: 0.6846 - accuracy: 0.5984 - precision: 0.6119 - recall: 0.5565 - true_positives: 1966.1471 - val_loss: 0.3140 - val_accuracy: 0.8723 - val_precision: 0.8648 - val_recall: 0.8830 - val_true_positives: 1471.0000\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 2/5\n",
            "67/67 [==============================] - 205s 3s/step - loss: 0.2598 - accuracy: 0.8940 - precision: 0.8868 - recall: 0.9034 - true_positives: 3116.0588 - val_loss: 0.2027 - val_accuracy: 0.9189 - val_precision: 0.9346 - val_recall: 0.9010 - val_true_positives: 1501.0000\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 3/5\n",
            "67/67 [==============================] - 205s 3s/step - loss: 0.1280 - accuracy: 0.9542 - precision: 0.9532 - recall: 0.9542 - true_positives: 3267.6618 - val_loss: 0.1823 - val_accuracy: 0.9297 - val_precision: 0.9655 - val_recall: 0.8914 - val_true_positives: 1485.0000\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 4/5\n",
            "67/67 [==============================] - 205s 3s/step - loss: 0.0586 - accuracy: 0.9822 - precision: 0.9848 - recall: 0.9792 - true_positives: 3374.1765 - val_loss: 0.2177 - val_accuracy: 0.9336 - val_precision: 0.9750 - val_recall: 0.8902 - val_true_positives: 1483.0000\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 5/5\n",
            "67/67 [==============================] - 206s 3s/step - loss: 0.0347 - accuracy: 0.9889 - precision: 0.9921 - recall: 0.9857 - true_positives: 3406.5441 - val_loss: 0.1908 - val_accuracy: 0.9432 - val_precision: 0.9420 - val_recall: 0.9448 - val_true_positives: 1574.0000\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f8074703650>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dkPWmTIRyQmf"
      },
      "source": [
        "Then the metrics are computed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r7u5J33-P6ot",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "375d5833-0cb5-44c8-a2b6-4a772067336b"
      },
      "source": [
        "model.metrics_names\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['loss', 'accuracy', 'precision', 'recall', 'true_positives']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z19GFYb2OuF-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3db2abdc-a931-4087-ca44-28469173e066"
      },
      "source": [
        "loss, accuracy, precision, recall, true_positives = model.evaluate(test_X, test_y, batch_size=batch_size)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "21/21 [==============================] - 15s 703ms/step - loss: 0.2085 - accuracy: 0.9440 - precision: 0.9368 - recall: 0.9541 - true_positives: 2017.0000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HeCiFkpmOzgB"
      },
      "source": [
        "mult_pr=precision*recall\n",
        "sum_pr=precision+recall\n",
        "div=mult_pr/sum_pr\n",
        "f1_score=2*div"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WgJGpdyzO2g7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43135b5e-cef3-4f3d-eb30-5a7f66807ca9"
      },
      "source": [
        "print('Loss:',loss)\n",
        "print('Accuracy:',accuracy)\n",
        "print('Precision:',precision)\n",
        "print('Recall:',recall)\n",
        "print('f1 score:',f1_score)\n",
        "print('True positives:',true_positives)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss: 0.20851683616638184\n",
            "Accuracy: 0.9439904093742371\n",
            "Precision: 0.9368323087692261\n",
            "Recall: 0.9541154503822327\n",
            "f1 score: 0.9453948961710928\n",
            "True positives: 2017.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-OYzySLtO41W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "716668f6-a87e-4859-e0dc-8a00ccc8fe3a"
      },
      "source": [
        "pred_y = model.predict(test_X, batch_size=batch_size)\n",
        "pred_y\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.0000000e+00],\n",
              "       [1.0000000e+00],\n",
              "       [9.9999440e-01],\n",
              "       ...,\n",
              "       [9.6881390e-04],\n",
              "       [9.9999964e-01],\n",
              "       [1.0000000e+00]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJItWu0q_XpO"
      },
      "source": [
        "pred_y2 = np.round(pred_y, decimals = 1)\n",
        "pred_y2=pred_y2.astype('int64')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9QgyNFnyW-z"
      },
      "source": [
        "And the confusion matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0bKKt4HfO7jS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47a3f42b-41a4-43e6-d1e8-68ed89b8e684"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "confusion_matrix(test_y, pred_y2)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1998,   48],\n",
              "       [ 222, 1892]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U1ZQ-ZfKypzx"
      },
      "source": [
        "Finally, false positives and false negatives are stored in two csv files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6klPNFuWNxQF"
      },
      "source": [
        "def getFP_FN_TP_lists(test_X, test_y, pred_y):\n",
        "    FP_text = []\n",
        "    FP_index = []\n",
        "    FN_text = []\n",
        "    FN_index = []\n",
        "    TP_text = []\n",
        "    TP_index = []\n",
        "    for i in range(len(test_y)):\n",
        "        if(pred_y2[i]==1 and test_y[test_y.index[i]]==0):\n",
        "            FP_text.append(test['text'][test_y.index[i]])\n",
        "            FP_index.append(test_y.index[i])\n",
        "        elif(pred_y2[i]==0 and test_y[test_y.index[i]]==1):\n",
        "            FN_text.append(test['text'][test_y.index[i]])\n",
        "            FN_index.append(test_y.index[i])\n",
        "        elif(pred_y2[i]==1 and test_y[test_y.index[i]]==1):\n",
        "            TP_text.append(test['text'][test_y.index[i]])\n",
        "            TP_index.append(test_y.index[i])        \n",
        "            \n",
        "    return FP_text,FP_index,FN_text,FN_index,TP_text,TP_index\n",
        "\n",
        "def getFP_FN_TP(test_X, test_y, pred_y2):\n",
        "    FP_text,FP_index,FN_text,FN_index,TP_text,TP_index = getFP_FN_TP_lists(test_X, test_y, pred_y2)\n",
        "    d_FP = {'FP_text':FP_text,'FP_index':FP_index}\n",
        "    df_FP = pd.DataFrame(d_FP)\n",
        "    d_FN = {'FN_text':FN_text,'FN_index':FN_index}\n",
        "    df_FN = pd.DataFrame(d_FN)\n",
        "    d_TP =  {'TP_text':TP_text,'TP_index':TP_index}\n",
        "    df_TP = pd.DataFrame(d_FP)\n",
        "    \n",
        "    return df_FP,df_FN,df_TP\n",
        "\n",
        "df_FP,df_FN, df_TP = getFP_FN_TP(test_X, test_y, pred_y2)\n",
        "df_FP.to_csv('FP_CNNglove.csv', index=True)\n",
        "df_FN.to_csv('FN_CNNglove.csv', index=True)\n",
        "df_TP.to_csv('TP_CNNglove.csv', index=True)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
